# COLEPA - Asistente Legal Gubernamental
# Backend FastAPI Mejorado para Consultas Legales Oficiales - VERSI√ìN PREMIUM

import os
import re
import time
import logging
from datetime import datetime
from typing import List, Dict, Optional, Any

from fastapi import FastAPI, HTTPException, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
import uvicorn

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Verificar y configurar OpenAI
try:
    from openai import OpenAI
    from dotenv import load_dotenv
    
    load_dotenv()
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    OPENAI_AVAILABLE = True
    logger.info("‚úÖ OpenAI configurado correctamente")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è OpenAI no disponible: {e}")
    OPENAI_AVAILABLE = False
    openai_client = None

# Importaciones locales con fallback
try:
    from app.vector_search import buscar_articulo_relevante, buscar_articulo_por_numero
    from app.prompt_builder import construir_prompt
    VECTOR_SEARCH_AVAILABLE = True
    logger.info("‚úÖ M√≥dulos de b√∫squeda vectorial cargados")
except ImportError:
    logger.warning("‚ö†Ô∏è M√≥dulos de b√∫squeda no encontrados, usando funciones mock")
    VECTOR_SEARCH_AVAILABLE = False
    
    def buscar_articulo_relevante(query_vector, collection_name):
        return {
            "pageContent": "Contenido de ejemplo del art√≠culo", 
            "nombre_ley": "C√≥digo Civil", 
            "numero_articulo": "123"
        }
    
    def buscar_articulo_por_numero(numero, collection_name):
        return {
            "pageContent": f"Contenido del art√≠culo {numero}", 
            "nombre_ley": "C√≥digo Civil", 
            "numero_articulo": str(numero)
        }
    
    def construir_prompt(contexto_legal, pregunta_usuario):
        return f"Contexto Legal: {contexto_legal}\n\nPregunta del Usuario: {pregunta_usuario}"

# ========== NUEVO: CLASIFICADOR INTELIGENTE ==========
try:
    from app.clasificador_inteligente import clasificar_y_procesar
    CLASIFICADOR_AVAILABLE = True
    logger.info("‚úÖ Clasificador inteligente cargado")
except ImportError:
    logger.warning("‚ö†Ô∏è Clasificador no encontrado, modo b√°sico")
    CLASIFICADOR_AVAILABLE = False
    
    def clasificar_y_procesar(texto):
        return {
            'tipo_consulta': 'consulta_legal',
            'respuesta_directa': None,
            'requiere_busqueda': True,
            'es_conversacional': False
        }

# === MODELOS PYDANTIC ===
class MensajeChat(BaseModel):
    role: str = Field(..., pattern="^(user|assistant|system)$")
    content: str = Field(..., min_length=1, max_length=3000)
    timestamp: Optional[datetime] = None

class ConsultaRequest(BaseModel):
    historial: List[MensajeChat] = Field(..., min_items=1, max_items=20)
    metadatos: Optional[Dict[str, Any]] = None

class FuenteLegal(BaseModel):
    ley: str
    articulo_numero: str
    libro: Optional[str] = None
    titulo: Optional[str] = None

class ConsultaResponse(BaseModel):
    respuesta: str
    fuente: Optional[FuenteLegal] = None
    recomendaciones: Optional[List[str]] = None
    tiempo_procesamiento: Optional[float] = None
    es_respuesta_oficial: bool = True

class StatusResponse(BaseModel):
    status: str
    timestamp: datetime
    version: str
    servicios: Dict[str, str]
    colecciones_disponibles: int

# ========== NUEVOS MODELOS PARA M√âTRICAS ==========
class MetricasCalidad(BaseModel):
    consulta_id: str
    tiene_contexto: bool
    relevancia_contexto: float
    longitud_respuesta: int
    tiempo_procesamiento: float
    codigo_identificado: str
    articulo_encontrado: Optional[str] = None

# === CONFIGURACI√ìN DEL SISTEMA ===
MAPA_COLECCIONES = {
    "C√≥digo Aduanero": "colepa_aduanero_maestro",
    "C√≥digo Civil": "colepa_civil_maestro", 
    "C√≥digo Electoral": "colepa_electoral_maestro",
    "C√≥digo Laboral": "colepa_laboral_maestro",
    "C√≥digo de la Ni√±ez y la Adolescencia": "colepa_ninezadolescencia_maestro",
    "C√≥digo de Organizaci√≥n Judicial": "colepa_organizacion_judicial_maestro",
    "C√≥digo Penal": "colepa_penal_maestro",
    "C√≥digo Procesal Civil": "colepa_procesal_civil_maestro",
    "C√≥digo Procesal Penal": "colepa_procesal_penal_maestro",
    "C√≥digo Sanitario": "colepa_sanitario_maestro"
}

PALABRAS_CLAVE_EXPANDIDAS = {
    "C√≥digo Civil": [
        "civil", "matrimonio", "divorcio", "propiedad", "contratos", "familia", 
        "herencia", "sucesi√≥n", "sociedad conyugal", "bien ganancial", "patria potestad",
        "tutela", "curatela", "adopci√≥n", "filiaci√≥n", "alimentos", "r√©gimen patrimonial",
        "esposo", "esposa", "c√≥nyuge", "pareja", "hijos", "padres"
    ],
    "C√≥digo Penal": [
        "penal", "delito", "crimen", "pena", "prisi√≥n", "robo", "homicidio", "hurto",
        "estafa", "violaci√≥n", "agresi√≥n", "lesiones", "amenaza", "extorsi√≥n", "secuestro",
        "narcotr√°fico", "corrupci√≥n", "fraude", "violencia dom√©stica", "femicidio",
        "pega", "golpea", "golpes", "maltrato", "abuso", "acoso", "persigue", "molesta",
        "choque", "chocaron", "atropello", "accidente", "atropell√≥"
    ],
    "C√≥digo Laboral": [
        "laboral", "trabajo", "empleado", "salario", "vacaciones", "despido", "contrato laboral",
        "indemnizaci√≥n", "aguinaldo", "licencia", "maternidad", "seguridad social", "sindicato",
        "huelga", "jornada laboral", "horas extras", "jubilaci√≥n", "accidente laboral",
        "jefe", "patr√≥n", "empleador", "trabajador", "sueldo"
    ],
    "C√≥digo Procesal Civil": [
        "proceso civil", "demanda", "juicio civil", "sentencia", "apelaci√≥n", "recurso",
        "prueba", "testigo", "peritaje", "embargo", "medida cautelar", "ejecuci√≥n",
        "da√±os", "perjuicios", "responsabilidad civil", "indemnizaci√≥n"
    ],
    "C√≥digo Procesal Penal": [
        "proceso penal", "acusaci√≥n", "juicio penal", "fiscal", "defensor", "imputado",
        "querella", "investigaci√≥n", "allanamiento", "detenci√≥n", "prisi√≥n preventiva",
        "denuncia", "denunciar", "comisar√≠a", "polic√≠a"
    ],
    "C√≥digo Aduanero": [
        "aduana", "aduanero", "importaci√≥n", "exportaci√≥n", "aranceles", "tributo aduanero", "mercanc√≠a",
        "declaraci√≥n aduanera", "r√©gimen aduanero", "zona franca", "contrabando", "dep√≥sito", "habilitaci√≥n"
    ],
    "C√≥digo Electoral": [
        "electoral", "elecciones", "voto", "candidato", "sufragio", "padr√≥n electoral",
        "tribunal electoral", "campa√±a electoral", "partido pol√≠tico", "referendum"
    ],
    "C√≥digo de la Ni√±ez y la Adolescencia": [
        "menor", "ni√±o", "adolescente", "tutela", "adopci√≥n", "menor infractor",
        "protecci√≥n integral", "derechos del ni√±o", "consejer√≠a", "medida socioeducativa",
        "hijo", "hija", "ni√±os", "ni√±as", "menores"
    ],
    "C√≥digo de Organizaci√≥n Judicial": [
        "judicial", "tribunal", "juez", "competencia", "jurisdicci√≥n", "corte suprema",
        "juzgado", "fuero", "instancia", "sala", "magistrado", "secretario judicial"
    ],
    "C√≥digo Sanitario": [
        "sanitario", "salud", "medicina", "hospital", "cl√≠nica", "medicamento",
        "profesional sanitario", "epidemia", "vacuna", "control sanitario"
    ]
}

# ========== CONFIGURACI√ìN DE TOKENS OPTIMIZADA ==========
MAX_TOKENS_INPUT_CONTEXTO = 400      # M√°ximo tokens para contexto legal
MAX_TOKENS_RESPUESTA = 300           # M√°ximo tokens para respuesta
MAX_TOKENS_SISTEMA = 180             # M√°ximo tokens para prompt sistema

# ========== PROMPT PREMIUM COMPACTO ==========
INSTRUCCION_SISTEMA_LEGAL_PREMIUM = """
COLEPA - Asistente jur√≠dico Paraguay. Respuesta obligatoria:

**DISPOSICI√ìN:** [Ley + Art√≠culo espec√≠fico]
**FUNDAMENTO:** [Texto normativo textual]  
**APLICACI√ìN:** [C√≥mo aplica a la consulta]

M√°ximo 250 palabras. Solo use contexto proporcionado. Terminolog√≠a jur√≠dica precisa.
"""

# ========== NUEVA FUNCI√ìN: VALIDADOR DE CONTEXTO ==========
def validar_calidad_contexto(contexto: Optional[Dict], pregunta: str) -> tuple[bool, float]:
    """
    Valida si el contexto encontrado es realmente relevante para la pregunta.
    Retorna (es_valido, score_relevancia)
    """
    if not contexto or not contexto.get("pageContent"):
        return False, 0.0
    
    try:
        texto_contexto = contexto.get("pageContent", "").lower()
        pregunta_lower = pregunta.lower()
        
        # Extraer palabras clave de la pregunta
        palabras_pregunta = set(re.findall(r'\b\w+\b', pregunta_lower))
        palabras_contexto = set(re.findall(r'\b\w+\b', texto_contexto))
        
        # Calcular intersecci√≥n
        interseccion = palabras_pregunta & palabras_contexto
        
        if len(palabras_pregunta) == 0:
            return False, 0.0
            
        score_basico = len(interseccion) / len(palabras_pregunta)
        
        # Bonus por palabras clave jur√≠dicas importantes
        palabras_juridicas = {"art√≠culo", "c√≥digo", "ley", "disposici√≥n", "norma", "legal"}
        bonus_juridico = len(interseccion & palabras_juridicas) * 0.1
        
        # Bonus por n√∫meros de art√≠culo coincidentes
        numeros_pregunta = set(re.findall(r'\d+', pregunta))
        numeros_contexto = set(re.findall(r'\d+', texto_contexto))
        bonus_numeros = len(numeros_pregunta & numeros_contexto) * 0.2
        
        score_final = score_basico + bonus_juridico + bonus_numeros
        
        # Umbral de calidad: debe tener al menos 30% de relevancia
        es_valido = score_final >= 0.3 and len(texto_contexto.strip()) >= 50
        
        logger.info(f"üéØ Validaci√≥n contexto - Score: {score_final:.2f}, V√°lido: {es_valido}")
        return es_valido, score_final
        
    except Exception as e:
        logger.error(f"‚ùå Error validando contexto: {e}")
        return False, 0.0

# ========== NUEVA FUNCI√ìN: B√öSQUEDA MULTI-M√âTODO ==========
def buscar_con_manejo_errores(pregunta: str, collection_name: str) -> Optional[Dict]:
    """
    B√∫squeda robusta con m√∫ltiples m√©todos y validaci√≥n de calidad.
    """
    contexto_final = None
    metodo_exitoso = None
    
    # M√©todo 1: B√∫squeda por n√∫mero de art√≠culo espec√≠fico
    numero_articulo = extraer_numero_articulo_mejorado(pregunta)
    if numero_articulo and VECTOR_SEARCH_AVAILABLE:
        try:
            logger.info(f"üéØ M√©todo 1: B√∫squeda por art√≠culo {numero_articulo}")
            contexto = buscar_articulo_por_numero(numero_articulo, collection_name)
            
            if contexto:
                es_valido, score = validar_calidad_contexto(contexto, pregunta)
                if es_valido:
                    contexto_final = contexto
                    metodo_exitoso = f"B√∫squeda exacta Art. {numero_articulo}"
                    logger.info(f"‚úÖ M√©todo 1 exitoso - Score: {score:.2f}")
                else:
                    logger.warning(f"‚ö†Ô∏è M√©todo 1 - Contexto no v√°lido (Score: {score:.2f})")
        except Exception as e:
            logger.error(f"‚ùå Error en M√©todo 1: {e}")
    
    # M√©todo 2: B√∫squeda sem√°ntica con embeddings
    if not contexto_final and OPENAI_AVAILABLE and VECTOR_SEARCH_AVAILABLE:
        try:
            logger.info("üîç M√©todo 2: B√∫squeda sem√°ntica con embeddings")
            
            # Optimizar consulta para embeddings
            consulta_optimizada = f"{pregunta} legislaci√≥n paraguay derecho"
            
            embedding_response = openai_client.embeddings.create(
                model="text-embedding-ada-002",
                input=consulta_optimizada
            )
            query_vector = embedding_response.data[0].embedding
            
            contexto = buscar_articulo_relevante(query_vector, collection_name)
            
            if contexto:
                es_valido, score = validar_calidad_contexto(contexto, pregunta)
                if es_valido and score >= 0.4:  # Umbral m√°s alto para sem√°ntica
                    contexto_final = contexto
                    metodo_exitoso = f"B√∫squeda sem√°ntica (Score: {score:.2f})"
                    logger.info(f"‚úÖ M√©todo 2 exitoso - Score: {score:.2f}")
                else:
                    logger.warning(f"‚ö†Ô∏è M√©todo 2 - Contexto no v√°lido (Score: {score:.2f})")
                    
        except Exception as e:
            logger.error(f"‚ùå Error en M√©todo 2: {e}")
    
    # M√©todo 3: B√∫squeda por palabras clave espec√≠ficas (fallback)
    if not contexto_final and numero_articulo and VECTOR_SEARCH_AVAILABLE:
        try:
            logger.info("üîÑ M√©todo 3: B√∫squeda fallback por palabras clave")
            
            # Crear vector dummy y usar filtros m√°s amplios
            contexto = buscar_articulo_relevante([0.1] * 1536, collection_name)
            
            if contexto:
                es_valido, score = validar_calidad_contexto(contexto, pregunta)
                if es_valido and score >= 0.2:  # Umbral m√°s bajo para fallback
                    contexto_final = contexto
                    metodo_exitoso = f"B√∫squeda fallback (Score: {score:.2f})"
                    logger.info(f"‚úÖ M√©todo 3 exitoso - Score: {score:.2f}")
                    
        except Exception as e:
            logger.error(f"‚ùå Error en M√©todo 3: {e}")
    
    if contexto_final:
        logger.info(f"üéâ Contexto encontrado usando: {metodo_exitoso}")
        return contexto_final
    else:
        logger.warning("‚ùå Ning√∫n m√©todo de b√∫squeda encontr√≥ contexto v√°lido")
        return None

# === CONFIGURACI√ìN DE FASTAPI ===
app = FastAPI(
    title="COLEPA - Asistente Legal Oficial",
    description="Sistema de consultas legales basado en la legislaci√≥n paraguaya",
    version="3.2.0-PREMIUM",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://www.colepa.com",
        "https://colepa.com", 
        "https://colepa-demo-2.vercel.app",
        "http://localhost:3000",
        "http://localhost:8080"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# ========== M√âTRICAS EN MEMORIA PARA DEMO ==========
metricas_sistema = {
    "consultas_procesadas": 0,
    "contextos_encontrados": 0,
    "tiempo_promedio": 0.0,
    "ultima_actualizacion": datetime.now()
}

# === FUNCIONES AUXILIARES MEJORADAS ===
def extraer_numero_articulo_mejorado(texto: str) -> Optional[int]:
    """
    Extracci√≥n mejorada y m√°s precisa de n√∫meros de art√≠culo
    """
    texto_lower = texto.lower()
    
    # Patrones m√°s espec√≠ficos y completos
    patrones = [
        r'art[i√≠]culo\s*(?:n[√∫u]mero\s*)?(\d+)',
        r'art\.?\s*(\d+)',
        r'art√≠culo\s*(\d+)',
        r'articulo\s*(\d+)',
        r'art\s+(\d+)',
        r'(?:^|\s)(\d+)(?:\s|$)',  # N√∫mero solo si est√° aislado
    ]
    
    for patron in patrones:
        matches = re.finditer(patron, texto_lower)
        for match in matches:
            try:
                numero = int(match.group(1))
                if 1 <= numero <= 9999:  # Rango razonable para art√≠culos
                    logger.info(f"üîç N√∫mero de art√≠culo extra√≠do: {numero} con patr√≥n: {patron}")
                    return numero
            except (ValueError, IndexError):
                continue
    
    logger.info(f"üîç No se encontr√≥ n√∫mero de art√≠culo en: {texto[:50]}...")
    return None

def clasificar_consulta_inteligente(pregunta: str) -> str:
    """
    Clasificaci√≥n inteligente mejorada con mejor scoring
    """
    pregunta_lower = pregunta.lower()
    scores = {}
    
    # B√∫squeda por palabras clave con peso ajustado
    for ley, palabras in PALABRAS_CLAVE_EXPANDIDAS.items():
        score = 0
        for palabra in palabras:
            if palabra in pregunta_lower:
                # Mayor peso para coincidencias exactas de palabras completas
                if f" {palabra} " in f" {pregunta_lower} ":
                    score += 5
                elif palabra in pregunta_lower:
                    score += 2
        
        if score > 0:
            scores[ley] = score
    
    # B√∫squeda por menciones expl√≠citas de c√≥digos (peso muy alto)
    for ley in MAPA_COLECCIONES.keys():
        ley_lower = ley.lower()
        # Buscar nombre completo
        if ley_lower in pregunta_lower:
            scores[ley] = scores.get(ley, 0) + 20
        
        # Buscar versiones sin "c√≥digo"
        ley_sin_codigo = ley_lower.replace("c√≥digo ", "").replace("c√≥digo de ", "")
        if ley_sin_codigo in pregunta_lower:
            scores[ley] = scores.get(ley, 0) + 15
    
    # Patrones espec√≠ficos mejorados para casos reales
    patrones_especiales = {
        r'violen(cia|to|tar)|agre(si√≥n|dir)|golpe|maltrato|femicidio|pega|abuso': "C√≥digo Penal",
        r'matrimonio|divorcio|esposo|esposa|c√≥nyuge|familia|pareja': "C√≥digo Civil", 
        r'trabajo|empleo|empleado|jefe|patr√≥n|salario|sueldo|laboral': "C√≥digo Laboral",
        r'menor|ni√±o|ni√±a|adolescente|hijo|hija|adopci√≥n': "C√≥digo de la Ni√±ez y la Adolescencia",
        r'elecci√≥n|elecciones|voto|votar|candidato|pol√≠tico|electoral': "C√≥digo Electoral",
        r'choque|chocaron|atropello|atropell√≥|accidente|da√±os|perjuicios': "C√≥digo Procesal Civil",
        r'denuncia|fiscal|delito|acusado|penal|proceso penal|comisar√≠a': "C√≥digo Procesal Penal",
        r'aduana|aduanero|importa|exporta|mercanc√≠a|dep√≥sito': "C√≥digo Aduanero",
        r'salud|medicina|m√©dico|hospital|sanitario': "C√≥digo Sanitario",
        r'acoso|persigue|molesta|hostiga': "C√≥digo Penal"
    }
    
    for patron, ley in patrones_especiales.items():
        if re.search(patron, pregunta_lower):
            scores[ley] = scores.get(ley, 0) + 12
    
    # Determinar la mejor clasificaci√≥n
    if scores:
        mejor_ley = max(scores.keys(), key=lambda k: scores[k])
        score_final = scores[mejor_ley]
        logger.info(f"üìö Consulta clasificada como: {mejor_ley} (score: {score_final})")
        return MAPA_COLECCIONES[mejor_ley]
    
    # Default: C√≥digo Civil (m√°s general)
    logger.info("üìö Consulta no clasificada espec√≠ficamente, usando C√≥digo Civil por defecto")
    return MAPA_COLECCIONES["C√≥digo Civil"]

def clasificar_consulta_con_ia_robusta(pregunta: str) -> str:
    """
    S√öPER ENRUTADOR: Clasificaci√≥n robusta usando IA con l√≠mites de tokens
    """
    if not OPENAI_AVAILABLE or not openai_client:
        logger.warning("‚ö†Ô∏è OpenAI no disponible, usando clasificaci√≥n b√°sica")
        return clasificar_consulta_inteligente(pregunta)
    
    # PROMPT ULTRA-COMPACTO PARA CLASIFICACI√ìN
    prompt_clasificacion = f"""Clasifica esta consulta legal paraguaya en uno de estos c√≥digos:

C√ìDIGOS:
1. C√≥digo Civil - matrimonio, divorcio, familia, propiedad, contratos
2. C√≥digo Penal - delitos, violencia, agresi√≥n, robo, homicidio  
3. C√≥digo Laboral - trabajo, empleo, salarios, despidos
4. C√≥digo Procesal Civil - demandas civiles, da√±os, perjuicios
5. C√≥digo Procesal Penal - denuncias penales, investigaciones
6. C√≥digo Aduanero - aduana, importaci√≥n, exportaci√≥n
7. C√≥digo Electoral - elecciones, votos, candidatos
8. C√≥digo de la Ni√±ez y la Adolescencia - menores, ni√±os
9. C√≥digo de Organizaci√≥n Judicial - tribunales, jueces
10. C√≥digo Sanitario - salud, medicina, hospitales

CONSULTA: "{pregunta[:150]}"

Responde solo el nombre exacto (ej: "C√≥digo Penal")"""

    try:
        response = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",  # Modelo m√°s econ√≥mico
            messages=[{"role": "user", "content": prompt_clasificacion}],
            temperature=0.1,
            max_tokens=20,  # ULTRA L√çMITE para clasificaci√≥n
            timeout=10  # Timeout reducido
        )
        
        codigo_identificado = response.choices[0].message.content.strip()
        
        # LOG DE TOKENS
        if hasattr(response, 'usage'):
            logger.info(f"üí∞ Clasificaci√≥n - Tokens: {response.usage.total_tokens}")
        
        # Mapear respuesta a colecci√≥n
        if codigo_identificado in MAPA_COLECCIONES:
            collection_name = MAPA_COLECCIONES[codigo_identificado]
            logger.info(f"üéØ IA clasific√≥: {codigo_identificado} ‚Üí {collection_name}")
            return collection_name
        else:
            # Fuzzy matching para nombres similares
            for codigo_oficial in MAPA_COLECCIONES.keys():
                if any(word in codigo_identificado.lower() for word in codigo_oficial.lower().split()):
                    collection_name = MAPA_COLECCIONES[codigo_oficial]
                    logger.info(f"üéØ IA clasific√≥ (fuzzy): {codigo_identificado} ‚Üí {codigo_oficial}")
                    return collection_name
            
            # Fallback
            logger.warning(f"‚ö†Ô∏è IA devolvi√≥ c√≥digo no reconocido: {codigo_identificado}")
            return clasificar_consulta_inteligente(pregunta)
            
    except Exception as e:
        logger.error(f"‚ùå Error en clasificaci√≥n con IA: {e}")
        return clasificar_consulta_inteligente(pregunta)

def truncar_contexto_inteligente(contexto: str, max_tokens: int = MAX_TOKENS_INPUT_CONTEXTO) -> str:
    """
    Trunca el contexto legal manteniendo las partes m√°s importantes
    """
    if not contexto:
        return ""
    
    # Estimaci√≥n: 1 token ‚âà 4 caracteres en espa√±ol
    max_chars = max_tokens * 4
    
    if len(contexto) <= max_chars:
        return contexto
    
    # Priorizar texto que contenga art√≠culos espec√≠ficos
    lineas = contexto.split('\n')
    lineas_prioritarias = []
    lineas_normales = []
    
    for linea in lineas:
        if any(palabra in linea.lower() for palabra in ['art√≠culo', 'art√≠culo', 'art.', 'establece', 'dispone']):
            lineas_prioritarias.append(linea)
        else:
            lineas_normales.append(linea)
    
    # Reconstruir con prioridades
    texto_final = '\n'.join(lineas_prioritarias)
    
    # Agregar l√≠neas normales si hay espacio
    chars_restantes = max_chars - len(texto_final)
    for linea in lineas_normales:
        if len(texto_final) + len(linea) + 1 <= max_chars:
            texto_final += '\n' + linea
        else:
            break
    
    # Si a√∫n es muy largo, truncar al final
    if len(texto_final) > max_chars:
        texto_final = texto_final[:max_chars-10] + "... [TEXTO TRUNCADO]"
    
    logger.info(f"üìè Contexto truncado: {len(contexto)} ‚Üí {len(texto_final)} chars")
    return texto_final

def generar_respuesta_legal_premium(historial: List[MensajeChat], contexto: Optional[Dict] = None) -> str:
    """
    Generaci√≥n de respuesta legal PREMIUM con l√≠mites estrictos de tokens
    """
    if not OPENAI_AVAILABLE or not openai_client:
        return generar_respuesta_con_contexto(historial[-1].content, contexto)
    
    try:
        pregunta_actual = historial[-1].content
        
        # Validar contexto antes de procesar
        if contexto:
            es_valido, score_relevancia = validar_calidad_contexto(contexto, pregunta_actual)
            if not es_valido:
                logger.warning(f"‚ö†Ô∏è Contexto no v√°lido (score: {score_relevancia:.2f}), generando respuesta sin contexto")
                contexto = None
        
        # Preparar mensajes para OpenAI con L√çMITES ESTRICTOS
        mensajes = [{"role": "system", "content": INSTRUCCION_SISTEMA_LEGAL_PREMIUM}]
        
        # Construcci√≥n del prompt con CONTROL DE TOKENS
        if contexto and contexto.get("pageContent"):
            ley = contexto.get('nombre_ley', 'Legislaci√≥n paraguaya')
            articulo = contexto.get('numero_articulo', 'N/A')
            contenido_legal = contexto.get('pageContent', '')
            
            # TRUNCAR CONTEXTO INTELIGENTEMENTE
            contenido_truncado = truncar_contexto_inteligente(contenido_legal)
            
            # PROMPT COMPACTO OPTIMIZADO
            prompt_profesional = f"""CONSULTA: {pregunta_actual[:200]}

NORMA: {ley} - Art. {articulo}
TEXTO: {contenido_truncado}

Responda en formato estructurado."""
            
            mensajes.append({"role": "user", "content": prompt_profesional})
            logger.info(f"üìñ Prompt generado - Chars: {len(prompt_profesional)}")
        else:
            # Sin contexto - RESPUESTA ULTRA COMPACTA
            prompt_sin_contexto = f"""CONSULTA: {pregunta_actual[:150]}

Sin normativa espec√≠fica encontrada. Respuesta profesional breve."""
            
            mensajes.append({"role": "user", "content": prompt_sin_contexto})
            logger.info("üìù Prompt sin contexto - Modo compacto")
        
        # Llamada a OpenAI con L√çMITES ESTRICTOS
        response = openai_client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=mensajes,
            temperature=0.1,
            max_tokens=MAX_TOKENS_RESPUESTA,  # L√çMITE ESTRICTO
            presence_penalty=0,
            frequency_penalty=0,
            timeout=25  # Timeout reducido
        )
        
        respuesta = response.choices[0].message.content
        
        # LOG DE TOKENS UTILIZADOS
        if hasattr(response, 'usage'):
            tokens_input = response.usage.prompt_tokens
            tokens_output = response.usage.completion_tokens
            tokens_total = response.usage.total_tokens
            logger.info(f"üí∞ Tokens utilizados - Input: {tokens_input}, Output: {tokens_output}, Total: {tokens_total}")
        
        logger.info("‚úÖ Respuesta premium generada con l√≠mites estrictos")
        return respuesta
        
    except Exception as e:
        logger.error(f"‚ùå Error con OpenAI en modo premium: {e}")
        return generar_respuesta_con_contexto(historial[-1].content, contexto)

def generar_respuesta_con_contexto(pregunta: str, contexto: Optional[Dict] = None) -> str:
    """
    Respuesta directa PREMIUM usando el contexto de Qdrant
    """
    if contexto and contexto.get("pageContent"):
        ley = contexto.get('nombre_ley', 'Legislaci√≥n paraguaya')
        articulo = contexto.get('numero_articulo', 'N/A')
        contenido = contexto.get('pageContent', '')
        
        # Formato profesional estructurado
        response = f"""**DISPOSICI√ìN LEGAL**
{ley}, Art√≠culo {articulo}

**FUNDAMENTO NORMATIVO**
{contenido}

**APLICACI√ìN JUR√çDICA**
La disposici√≥n citada responde directamente a la consulta planteada sobre "{pregunta}".

---
*Fuente: {ley}, Art√≠culo {articulo}*
*Para asesoramiento espec√≠fico, consulte con profesional del derecho especializado.*"""
        
        logger.info(f"‚úÖ Respuesta premium generada con contexto: {ley} Art. {articulo}")
        return response
    else:
        return f"""**CONSULTA LEGAL - INFORMACI√ìN NO DISPONIBLE**

No se encontr√≥ disposici√≥n normativa espec√≠fica aplicable a: "{pregunta}"

**RECOMENDACIONES PROCESALES:**
1. **Reformule la consulta** con mayor especificidad t√©cnica
2. **Especifique el cuerpo normativo** de su inter√©s (C√≥digo Civil, Penal, etc.)
3. **Indique n√∫mero de art√≠culo** si conoce la disposici√≥n espec√≠fica

**√ÅREAS DE CONSULTA DISPONIBLES:**
- Normativa civil (familia, contratos, propiedad)
- Normativa penal (delitos, procedimientos)
- Normativa laboral (relaciones de trabajo)
- Normativa procesal (procedimientos judiciales)

*Para consultas espec√≠ficas sobre casos particulares, dir√≠jase a profesional del derecho competente.*"""

def extraer_fuente_legal(contexto: Optional[Dict]) -> Optional[FuenteLegal]:
    """
    Extrae informaci√≥n de la fuente legal del contexto
    """
    if not contexto:
        return None
    
    return FuenteLegal(
        ley=contexto.get("nombre_ley", "No especificada"),
        articulo_numero=str(contexto.get("numero_articulo", "N/A")),
        libro=contexto.get("libro"),
        titulo=contexto.get("titulo")
    )

def actualizar_metricas(tiene_contexto: bool, tiempo_procesamiento: float, codigo: str, articulo: Optional[str] = None):
    """
    Actualiza m√©tricas del sistema para monitoreo en tiempo real
    """
    global metricas_sistema
    
    metricas_sistema["consultas_procesadas"] += 1
    if tiene_contexto:
        metricas_sistema["contextos_encontrados"] += 1
    
    # Actualizar tiempo promedio
    total_consultas = metricas_sistema["consultas_procesadas"]
    tiempo_anterior = metricas_sistema["tiempo_promedio"]
    metricas_sistema["tiempo_promedio"] = ((tiempo_anterior * (total_consultas - 1)) + tiempo_procesamiento) / total_consultas
    
    metricas_sistema["ultima_actualizacion"] = datetime.now()
    
    logger.info(f"üìä M√©tricas actualizadas - Consultas: {total_consultas}, Contextos: {metricas_sistema['contextos_encontrados']}")

# === MIDDLEWARE ===
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    client_ip = request.client.host
    logger.info(f"üì• {request.method} {request.url.path} - IP: {client_ip}")
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(f"üì§ {response.status_code} - {process_time:.2f}s")
    
    return response

# === ENDPOINTS ===
@app.get("/", response_model=StatusResponse)
async def sistema_status():
    """Estado del sistema COLEPA"""
    return StatusResponse(
        status="‚úÖ Sistema COLEPA Premium Operativo",
        timestamp=datetime.now(),
        version="3.2.0-PREMIUM",
        servicios={
            "openai": "disponible" if OPENAI_AVAILABLE else "no disponible",
            "busqueda_vectorial": "disponible" if VECTOR_SEARCH_AVAILABLE else "modo_demo",
            "base_legal": "legislaci√≥n paraguaya completa",
            "modo": "PREMIUM - Demo Congreso Nacional"
        },
        colecciones_disponibles=len(MAPA_COLECCIONES)
    )

@app.get("/api/health")
async def health_check():
    """Verificaci√≥n de salud detallada"""
    health_status = {
        "sistema": "operativo",
        "timestamp": datetime.now().isoformat(),
        "version": "3.2.0-PREMIUM",
        "modo": "Demo Congreso Nacional",
        "servicios": {
            "openai": "‚ùå no disponible",
            "qdrant": "‚ùå no disponible" if not VECTOR_SEARCH_AVAILABLE else "‚úÖ operativo",
            "base_legal": "‚úÖ cargada",
            "validacion_contexto": "‚úÖ activa",
            "busqueda_multi_metodo": "‚úÖ activa"
        }
    }
    
    if OPENAI_AVAILABLE and openai_client:
        try:
            # Test m√≠nimo de OpenAI
            openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": "test"}],
                max_tokens=1,
                timeout=10
            )
            health_status["servicios"]["openai"] = "‚úÖ operativo"
        except Exception as e:
            health_status["servicios"]["openai"] = f"‚ùå error: {str(e)[:50]}"
    
    return health_status

@app.get("/api/codigos")
async def listar_codigos_legales():
    """Lista todos los c√≥digos legales disponibles"""
    return {
        "codigos_disponibles": list(MAPA_COLECCIONES.keys()),
        "total_codigos": len(MAPA_COLECCIONES),
        "descripcion": "C√≥digos legales completos de la Rep√∫blica del Paraguay",
        "ultima_actualizacion": "2024",
        "cobertura": "Legislaci√≥n nacional vigente",
        "modo": "PREMIUM - Optimizado para profesionales del derecho"
    }

# ========== NUEVO ENDPOINT: M√âTRICAS CON TOKENS ==========
@app.get("/api/metricas")
async def obtener_metricas():
    """M√©tricas del sistema con tracking de tokens para control de costos"""
    global metricas_sistema
    
    # Calcular porcentaje de √©xito
    total_consultas = metricas_sistema["consultas_procesadas"]
    contextos_encontrados = metricas_sistema["contextos_encontrados"]
    
    porcentaje_exito = (contextos_encontrados / total_consultas * 100) if total_consultas > 0 else 0
    
    return {
        "estado_sistema": "‚úÖ PREMIUM OPERATIVO",
        "version": "3.2.0-PREMIUM-OPTIMIZADO",
        "timestamp": datetime.now().isoformat(),
        "metricas": {
            "total_consultas_procesadas": total_consultas,
            "contextos_legales_encontrados": contextos_encontrados,
            "porcentaje_exito": round(porcentaje_exito, 1),
            "tiempo_promedio_respuesta": round(metricas_sistema["tiempo_promedio"], 2),
            "ultima_actualizacion": metricas_sistema["ultima_actualizacion"].isoformat()
        },
        "optimizacion_tokens": {
            "max_tokens_respuesta": MAX_TOKENS_RESPUESTA,
            "max_tokens_contexto": MAX_TOKENS_INPUT_CONTEXTO,
            "max_tokens_sistema": MAX_TOKENS_SISTEMA,
            "modelo_clasificacion": "gpt-3.5-turbo (econ√≥mico)",
            "modelo_respuesta": "gpt-4-turbo-preview (calidad)"
        },
        "configuracion": {
            "validacion_contexto_activa": True,
            "busqueda_multi_metodo": True,
            "formato_profesional": True,
            "control_costos_activo": True,
            "optimizado_para": "Congreso Nacional de Paraguay"
        }
    }

# ========== NUEVO ENDPOINT: TEST OPENAI ==========
@app.get("/api/test-openai")
async def test_openai_connection():
    """Test de conexi√≥n con OpenAI para diagn√≥stico"""
    if not OPENAI_AVAILABLE or not openai_client:
        return {
            "estado": "‚ùå OpenAI no disponible",
            "error": "Cliente OpenAI no inicializado",
            "recomendacion": "Verificar OPENAI_API_KEY en variables de entorno"
        }
    
    try:
        start_time = time.time()
        
        response = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Test de conexi√≥n COLEPA"}],
            max_tokens=10,
            timeout=10
        )
        
        tiempo_respuesta = time.time() - start_time
        
        return {
            "estado": "‚úÖ OpenAI operativo",
            "modelo": "gpt-3.5-turbo",
            "tiempo_respuesta": round(tiempo_respuesta, 2),
            "respuesta_test": response.choices[0].message.content,
            "tokens_utilizados": response.usage.total_tokens if hasattr(response, 'usage') else 0
        }
        
    except Exception as e:
        return {
            "estado": "‚ùå Error en OpenAI",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ========== ENDPOINT PRINCIPAL OPTIMIZADO PREMIUM ==========
@app.post("/api/consulta", response_model=ConsultaResponse)
async def procesar_consulta_legal_premium(
    request: ConsultaRequest, 
    background_tasks: BackgroundTasks
):
    """
    Endpoint principal PREMIUM para consultas legales oficiales del Congreso Nacional
    """
    start_time = time.time()
    
    try:
        historial = request.historial
        pregunta_actual = historial[-1].content
        
        # ========== L√çMITE DE HISTORIAL PARA EVITAR ERROR 422 ==========
        MAX_HISTORIAL = 3  # Solo √∫ltimos 3 mensajes para modo premium
        if len(historial) > MAX_HISTORIAL:
            historial_limitado = historial[-MAX_HISTORIAL:]
            logger.info(f"‚ö†Ô∏è Historial limitado a {len(historial_limitado)} mensajes (modo premium)")
        else:
            historial_limitado = historial
        
        logger.info(f"üèõÔ∏è Nueva consulta PREMIUM: {pregunta_actual[:100]}...")
        
        # ========== CLASIFICACI√ìN INTELIGENTE ==========
        if CLASIFICADOR_AVAILABLE:
            logger.info("üß† Iniciando clasificaci√≥n inteligente premium...")
            clasificacion = clasificar_y_procesar(pregunta_actual)
            
            # Si es una consulta conversacional
            if clasificacion['es_conversacional'] and clasificacion['respuesta_directa']:
                logger.info("üí¨ Respuesta conversacional directa...")
                
                tiempo_procesamiento = time.time() - start_time
                actualizar_metricas(False, tiempo_procesamiento, "conversacional")
                
                return ConsultaResponse(
                    respuesta=clasificacion['respuesta_directa'],
                    fuente=None,
                    recomendaciones=None,
                    tiempo_procesamiento=round(tiempo_procesamiento, 2),
                    es_respuesta_oficial=True
                )
            
            # Si no requiere b√∫squeda (tema no legal)
            if not clasificacion['requiere_busqueda']:
                logger.info("üö´ Consulta no legal, redirigiendo profesionalmente...")
                
                respuesta_profesional = """**CONSULTA FUERA DEL √ÅMBITO LEGAL**

COLEPA se especializa exclusivamente en normativa jur√≠dica paraguaya. La consulta planteada no corresponde al √°mbito de aplicaci√≥n del sistema.

**√ÅMBITOS DE COMPETENCIA:**
- Legislaci√≥n civil, penal y procesal
- Normativa laboral y administrativa  
- C√≥digos especializados (aduanero, electoral, sanitario)
- Organizaci√≥n judicial

Para consultas de otra naturaleza, dir√≠jase a los servicios especializados correspondientes."""
                
                tiempo_procesamiento = time.time() - start_time
                actualizar_metricas(False, tiempo_procesamiento, "no_legal")
                
                return ConsultaResponse(
                    respuesta=respuesta_profesional,
                    fuente=None,
                    recomendaciones=None,
                    tiempo_procesamiento=round(tiempo_procesamiento, 2),
                    es_respuesta_oficial=True
                )
        
        # ========== CLASIFICACI√ìN Y B√öSQUEDA PREMIUM ==========
        collection_name = clasificar_consulta_con_ia_robusta(pregunta_actual)
        logger.info(f"üìö C√≥digo legal identificado (PREMIUM): {collection_name}")
        
        # ========== B√öSQUEDA MULTI-M√âTODO CON VALIDACI√ìN ==========
        contexto = None
        if VECTOR_SEARCH_AVAILABLE:
            contexto = buscar_con_manejo_errores(pregunta_actual, collection_name)
        
        # Validar contexto final con est√°ndares premium
        contexto_valido = False
        if contexto and isinstance(contexto, dict) and contexto.get("pageContent"):
            es_valido, score_relevancia = validar_calidad_contexto(contexto, pregunta_actual)
            if es_valido and score_relevancia >= 0.3:  # Umbral premium
                contexto_valido = True
                logger.info(f"üìñ Contexto PREMIUM validado:")
                logger.info(f"   - Ley: {contexto.get('nombre_ley', 'N/A')}")
                logger.info(f"   - Art√≠culo: {contexto.get('numero_articulo', 'N/A')}")
                logger.info(f"   - Score relevancia: {score_relevancia:.2f}")
            else:
                logger.warning(f"‚ùå Contexto no cumple est√°ndares premium (score: {score_relevancia:.2f})")
                contexto = None
        else:
            logger.warning("‚ùå No se encontr√≥ contexto legal para modo premium")
        
        # ========== GENERACI√ìN DE RESPUESTA PREMIUM ==========
        respuesta = generar_respuesta_legal_premium(historial_limitado, contexto)
        
        # ========== PREPARAR RESPUESTA ESTRUCTURADA ==========
        tiempo_procesamiento = time.time() - start_time
        fuente = extraer_fuente_legal(contexto)
        
        # Actualizar m√©tricas del sistema
        codigo_identificado = "desconocido"
        for nombre_codigo, collection in MAPA_COLECCIONES.items():
            if collection == collection_name:
                codigo_identificado = nombre_codigo
                break
        
        articulo_encontrado = contexto.get("numero_articulo") if contexto else None
        actualizar_metricas(contexto_valido, tiempo_procesamiento, codigo_identificado, articulo_encontrado)
        
        response_data = ConsultaResponse(
            respuesta=respuesta,
            fuente=fuente,
            recomendaciones=None,  # Modo premium sin recomendaciones autom√°ticas
            tiempo_procesamiento=round(tiempo_procesamiento, 2),
            es_respuesta_oficial=True
        )
        
        logger.info(f"‚úÖ Consulta PREMIUM procesada exitosamente en {tiempo_procesamiento:.2f}s")
        logger.info(f"üéØ Contexto encontrado: {contexto_valido}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"‚ùå Error procesando consulta premium: {e}")
        
        # Actualizar m√©tricas de error
        tiempo_procesamiento = time.time() - start_time
        actualizar_metricas(False, tiempo_procesamiento, "error")
        
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Error interno del sistema premium",
                "mensaje": "No fue posible procesar su consulta legal en este momento",
                "recomendacion": "Intente nuevamente en unos momentos",
                "codigo_error": str(e)[:100],
                "timestamp": datetime.now().isoformat()
            }
        )

# === MANEJO DE ERRORES ===
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": True,
            "status_code": exc.status_code,
            "detalle": exc.detail,
            "timestamp": datetime.now().isoformat(),
            "mensaje_usuario": "Ha ocurrido un error procesando su consulta legal",
            "version": "3.2.0-PREMIUM"
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(f"‚ùå Error no controlado en modo premium: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": True,
            "status_code": 500,
            "detalle": "Error interno del servidor premium",
            "timestamp": datetime.now().isoformat(),
            "mensaje_usuario": "El sistema premium est√° experimentando dificultades t√©cnicas",
            "version": "3.2.0-PREMIUM"
        }
    )

# === PUNTO DE ENTRADA ===
if __name__ == "__main__":
    logger.info("üöÄ Iniciando COLEPA PREMIUM - Sistema Legal Gubernamental v3.2.0")
    logger.info("üèõÔ∏è Optimizado para Demo Congreso Nacional de Paraguay")
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=int(os.getenv("PORT", 8000)),
        reload=False,  # Deshabilitado en producci√≥n
        log_level="info"
    )
