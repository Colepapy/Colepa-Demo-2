yield f"data: {json.dumps(data)}\n\n"
            await asyncio.sleep(0.3)  # Simular tiempo de procesamiento
    
    return StreamingResponse(evento_demo(), headers=headers, media_type="text/plain")

# ========== NUEVO ENDPOINT: SUGERENCIAS INTELIGENTES ==========
@app.get("/api/sugerencias")
async def obtener_sugerencias_inteligentes(
    q: str = "",
    limite: int = 8,
    codigo: str = None
):
    """
    Endpoint para obtener sugerencias inteligentes de consultas
    
    Par√°metros:
    - q: Texto parcial para buscar sugerencias (opcional)
    - limite: N√∫mero m√°ximo de sugerencias (default: 8)
    - codigo: Filtrar por c√≥digo legal espec√≠fico (opcional)
    """
    try:
        if codigo and codigo in MAPA_COLECCIONES:
            # Sugerencias espec√≠ficas de un c√≥digo
            sugerencias_codigo = sugerencias_manager.obtener_sugerencias_por_codigo(codigo, limite)
            return {
                "sugerencias": [
                    {
                        "texto": sugerencia,
                        "codigo": codigo,
                        "tipo": "codigo_especifico"
                    }
                    for sugerencia in sugerencias_codigo
                ],
                "total": len(sugerencias_codigo),
                "filtro_aplicado": codigo,
                "timestamp": datetime.now().isoformat()
            }
        else:
            # Sugerencias generales o por texto parcial
            sugerencias = sugerencias_manager.obtener_sugerencias(q, limite)
            
            return {
                "sugerencias": sugerencias,
                "total": len(sugerencias),
                "query": q if q else "consultas_populares",
                "timestamp": datetime.now().isoformat(),
                "codigos_disponibles": list(MAPA_COLECCIONES.keys())
            }
            
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo sugerencias: {e}")
        return {
            "sugerencias": [],
            "error": "Error interno obteniendo sugerencias",
            "timestamp": datetime.now().isoformat()
        }

@app.get("/api/sugerencias/stats")
async def obtener_estadisticas_sugerencias():
    """Estad√≠sticas del sistema de sugerencias"""
    return {
        "status": "‚úÖ Sistema de sugerencias operativo",
        "timestamp": datetime.now().isoformat(),
        "estadisticas": sugerencias_manager.get_stats(),
        "codigos_con_sugerencias": list(sugerencias_manager.sugerencias_por_codigo.keys()),
        "funciones_disponibles": [
            "Auto-completar consultas",
            "Sugerencias por c√≥digo legal",
            "Tracking de consultas frecuentes",
            "Relevancia inteligente"
        ]
    }

@app.get("/dashboard-test")
async def test_dashboard():
    return {"mensaje": "Dashboard funcionando", "status": "OK"}

# ========== NUEVO ENDPOINT: DASHBOARD VISUAL PARA DEMO ==========
@app.get("/api/dashboard")
async def dashboard_visual_demo():
    """
    Dashboard visual impresionante para demo del Congreso Nacional
    Muestra m√©tricas en tiempo real con gr√°ficos animados
    """
    global metricas_sistema
    
    # Obtener todas las m√©tricas actuales
    cache_stats = cache_manager.get_stats()
    circuit_stats = circuit_breaker.get_status()
    
    # Calcular m√©tricas impresionantes
    total_consultas = metricas_sistema["consultas_procesadas"]
    porcentaje_exito = (metricas_sistema["contextos_encontrados"] / total_consultas * 100) if total_consultas > 0 else 0
    
    # HTML completo con CSS y JavaScript para gr√°ficos
    html_dashboard = f"""
<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>COLEPA - Dashboard Congreso Nacional</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.9.1/chart.min.js"></script>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            min-height: 100vh;
            padding: 20px;
        }}
        .header {{
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background: rgba(255,255,255,0.1);
            border-radius: 15px;
            backdrop-filter: blur(10px);
        }}
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
            color: #ffd700;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);
        }}
        .header p {{
            font-size: 1.2em;
            opacity: 0.9;
        }}
        .dashboard-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .metric-card {{
            background: rgba(255,255,255,0.15);
            border-radius: 15px;
            padding: 25px;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255,255,255,0.2);
            transition: transform 0.3s ease;
        }}
        .metric-card:hover {{
            transform: translateY(-5px);
        }}
        .metric-title {{
            font-size: 1.1em;
            margin-bottom: 15px;
            color: #ffd700;
            font-weight: bold;
        }}
        .metric-value {{
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 10px;
            color: #00ff88;
            text-shadow: 1px 1px 2px rgba(0,0,0,0.3);
        }}
        .metric-label {{
            font-size: 0.9em;
            opacity: 0.8;
        }}
        .chart-container {{
            background: rgba(255,255,255,0.15);
            border-radius: 15px;
            padding: 25px;
            margin-bottom: 20px;
            backdrop-filter: blur(10px);
        }}
        .status-indicator {{
            display: inline-block;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            margin-right: 8px;
        }}
        .status-ok {{ background-color: #00ff88; }}
        .status-warning {{ background-color: #ffd700; }}
        .status-error {{ background-color: #ff4757; }}
        .refresh-button {{
            position: fixed;
            top: 20px;
            right: 20px;
            background: #ffd700;
            color: #1e3c72;
            border: none;
            padding: 12px 24px;
            border-radius: 25px;
            font-weight: bold;
            cursor: pointer;
            font-size: 14px;
            transition: all 0.3s ease;
        }}
        .refresh-button:hover {{
            background: #ffed4e;
            transform: scale(1.05);
        }}
        .system-info {{
            background: rgba(255,255,255,0.1);
            border-radius: 15px;
            padding: 20px;
            margin-top: 20px;
        }}
    </style>
</head>
<body>
    <button class="refresh-button" onclick="location.reload()">üîÑ Actualizar</button>
    
    <div class="header">
        <h1>üèõÔ∏è COLEPA PREMIUM</h1>
        <p>Dashboard Operacional - Congreso Nacional de Paraguay</p>
        <p>Sistema Legal Gubernamental v3.3.0-PREMIUM-CACHE-RETRY</p>
    </div>

    <div class="dashboard-grid">
        <div class="metric-card">
            <div class="metric-title">üìä CONSULTAS PROCESADAS</div>
            <div class="metric-value">{total_consultas:,}</div>
            <div class="metric-label">Total desde inicializaci√≥n</div>
        </div>
        
        <div class="metric-card">
            <div class="metric-title">‚úÖ TASA DE √âXITO</div>
            <div class="metric-value">{porcentaje_exito:.1f}%</div>
            <div class="metric-label">Contexto legal encontrado</div>
        </div>
        
        <div class="metric-card">
            <div class="metric-title">‚ö° VELOCIDAD PROMEDIO</div>
            <div class="metric-value">{metricas_sistema['tiempo_promedio']:.2f}s</div>
            <div class="metric-label">Tiempo de respuesta</div>
        </div>
        
        <div class="metric-card">
            <div class="metric-title">üöÄ CACHE HIT RATE</div>
            <div class="metric-value">{cache_stats['hit_rate_percentage']:.1f}%</div>
            <div class="metric-label">Consultas instant√°neas</div>
        </div>
        
        <div class="metric-card">
            <div class="metric-title">üíæ CACHE ENTRADAS</div>
            <div class="metric-value">{sum(cache_stats['entradas_cache'].values()):,}</div>
            <div class="metric-label">Total en memoria</div>
        </div>
        
        <div class="metric-card">
            <div class="metric-title">üõ°Ô∏è CIRCUIT BREAKER</div>
            <div class="metric-value">
                <span class="status-indicator {'status-ok' if circuit_stats['openai_disponible'] else 'status-error'}"></span>
                {'ACTIVO' if circuit_stats['openai_disponible'] else 'PROTEGIDO'}
            </div>
            <div class="metric-label">Estado de protecci√≥n</div>
        </div>
    </div>

    <div class="chart-container">
        <h3 style="margin-bottom: 20px; color: #ffd700;">üìà Distribuci√≥n de Cache por Nivel</h3>
        <canvas id="cacheChart" width="400" height="200"></canvas>
    </div>

    <div class="chart-container">
        <h3 style="margin-bottom: 20px; color: #ffd700;">üéØ M√©tricas de Rendimiento</h3>
        <canvas id="performanceChart" width="400" height="200"></canvas>
    </div>

    <div class="system-info">
        <h3 style="color: #ffd700; margin-bottom: 15px;">üîß Informaci√≥n del Sistema</h3>
        <p><strong>Versi√≥n:</strong> COLEPA v3.3.0-PREMIUM-CACHE-RETRY</p>
        <p><strong>√öltima actualizaci√≥n:</strong> {metricas_sistema['ultima_actualizacion'].strftime('%d/%m/%Y %H:%M:%S')}</p>
        <p><strong>Memoria Cache:</strong> {cache_stats['memoria_estimada_mb']:.1f} MB / {cache_stats['limite_memoria_mb']:.1f} MB</p>
        <p><strong>OpenAI GPT-4:</strong> <span class="status-indicator {'status-ok' if circuit_stats['gpt4_disponible'] else 'status-warning'}"></span>{'Disponible' if circuit_stats['gpt4_disponible'] else 'Fallback activo'}</p>
        <p><strong>OpenAI GPT-3.5:</strong> <span class="status-indicator {'status-ok' if circuit_stats['gpt35_disponible'] else 'status-warning'}"></span>{'Disponible' if circuit_stats['gpt35_disponible'] else 'Fallback activo'}</p>
        <p><strong>Retry Logic:</strong> <span class="status-indicator status-ok"></span>3 intentos autom√°ticos</p>
    </div>

    <script>
        // Gr√°fico de Cache
        const cacheCtx = document.getElementById('cacheChart').getContext('2d');
        new Chart(cacheCtx, {{
            type: 'doughnut',
            data: {{
                labels: ['Clasificaciones', 'Contextos', 'Respuestas'],
                datasets: [{{
                    data: [{cache_stats['entradas_cache']['clasificaciones']}, {cache_stats['entradas_cache']['contextos']}, {cache_stats['entradas_cache']['respuestas']}],
                    backgroundColor: ['#ff6b6b', '#4ecdc4', '#45b7d1'],
                    borderWidth: 2,
                    borderColor: '#ffffff'
                }}]
            }},
            options: {{
                responsive: true,
                plugins: {{
                    legend: {{
                        labels: {{
                            color: 'white',
                            font: {{
                                size: 14
                            }}
                        }}
                    }}
                }}
            }}
        }});

        // Gr√°fico de Rendimiento
        const perfCtx = document.getElementById('performanceChart').getContext('2d');
        new Chart(perfCtx, {{
            type: 'bar',
            data: {{
                labels: ['Cache Hits', 'Cache Misses', 'Consultas Exitosas', 'Tiempo Promedio (x10)'],
                datasets: [{{
                    label: 'M√©tricas',
                    data: [{cache_stats['total_hits']}, {cache_stats['total_misses']}, {metricas_sistema['contextos_encontrados']}, {metricas_sistema['tiempo_promedio'] * 10}],
                    backgroundColor: ['#00ff88', '#ffd700', '#45b7d1', '#ff6b6b'],
                    borderWidth: 2,
                    borderColor: '#ffffff'
                }}]
            }},
            options: {{
                responsive: true,
                plugins: {{
                    legend: {{
                        labels: {{
                            color: 'white'
                        }}
                    }}
                }},
                scales: {{
                    y: {{
                        ticks: {{
                            color: 'white'
                        }},
                        grid: {{
                            color: 'rgba(255,255,255,0.1)'
                        }}
                    }},
                    x: {{
                        ticks: {{
                            color: 'white'
                        }},
                        grid: {{
                            color: 'rgba(255,255,255,0.1)'
                        }}
                    }}
                }}
            }}
        }});

        // Auto-refresh cada 30 segundos
        setTimeout(() => location.reload(), 30000);
    </script>
</body>
</html>
    """
    
    return HTMLResponse(content=html_dashboard)

# ========== NUEVO ENDPOINT: TEST OPENAI ==========
@app.get("/api/test-openai")
async def test_openai_connection():
    """Test de conexi√≥n con OpenAI para diagn√≥stico CON RETRY"""
    if not OPENAI_AVAILABLE or not openai_client:
        return {
            "estado": "‚ùå OpenAI no disponible",
            "error": "Cliente OpenAI no inicializado",
            "recomendacion": "Verificar OPENAI_API_KEY en variables de entorno"
        }
    
    try:
        start_time = time.time()
        
        response = llamar_openai_con_retry(
            modelo="gpt-3.5-turbo",
            mensajes=[{"role": "user", "content": "Test de conexi√≥n COLEPA"}],
            max_tokens=10,
            timeout=10
        )
        
        tiempo_respuesta = time.time() - start_time
        
        return {
            "estado": "‚úÖ OpenAI operativo",
            "modelo": "gpt-3.5-turbo",
            "tiempo_respuesta": round(tiempo_respuesta, 2),
            "respuesta_test": response.choices[0].message.content,
            "tokens_utilizados": response.usage.total_tokens if hasattr(response, 'usage') else 0,
            "cache_activo": "‚úÖ Cache de 3 niveles operativo",
            "retry_logic": "‚úÖ Reintentos autom√°ticos funcionando"
        }
        
    except Exception as e:
        return {
            "estado": "‚ùå Error en OpenAI",
            "error": str(e),
            "timestamp": datetime.now().isoformat(),
            "nota": "Fallbacks autom√°ticos activados"
        }

# ========== ENDPOINT PRINCIPAL OPTIMIZADO PREMIUM CON CACHE Y RETRY ==========
@app.post("/api/consulta", response_model=ConsultaResponse)
async def procesar_consulta_legal_premium(
    request: ConsultaRequest, 
    background_tasks: BackgroundTasks
):
    """
    Endpoint principal PREMIUM para consultas legales oficiales del Congreso Nacional
    AHORA CON CACHE INTELIGENTE DE 3 NIVELES + RETRY LOGIC + CIRCUIT BREAKER
    """
    start_time = time.time()
    
    try:
        historial = request.historial
        pregunta_actual = historial[-1].content
        
        # ========== L√çMITE DE HISTORIAL PARA EVITAR ERROR 422 ==========
        MAX_HISTORIAL = 3  # Solo √∫ltimos 3 mensajes para modo premium
        if len(historial) > MAX_HISTORIAL:
            historial_limitado = historial[-MAX_HISTORIAL:]
            logger.info(f"‚ö†Ô∏è Historial limitado a {len(historial_limitado)} mensajes (modo premium)")
        else:
            historial_limitado = historial
        
        # ========== LOGGING CON EVENTOS DE PROGRESO ==========
        logger.info(f"üèõÔ∏è Nueva consulta PREMIUM CON CACHE + RETRY: {pregunta_actual[:100]}...")
        
        # En una implementaci√≥n completa, aqu√≠ se enviar√≠an eventos reales
        # Por ahora solo logging mejorado para indicar progreso
        
        # ========== CLASIFICACI√ìN INTELIGENTE ==========
        if CLASIFICADOR_AVAILABLE:
            logger.info("üß† Iniciando clasificaci√≥n inteligente premium...")
            clasificacion = clasificar_y_procesar(pregunta_actual)
            
            # Si es una consulta conversacional
            if clasificacion['es_conversacional'] and clasificacion['respuesta_directa']:
                logger.info("üí¨ Respuesta conversacional directa...")
                
                tiempo_procesamiento = time.time() - start_time
                actualizar_metricas(False, tiempo_procesamiento, "conversacional")
                
                return ConsultaResponse(
                    respuesta=clasificacion['respuesta_directa'],
                    fuente=None,
                    recomendaciones=None,
                    tiempo_procesamiento=round(tiempo_procesamiento, 2),
                    es_respuesta_oficial=True
                )
            
            # Si no requiere b√∫squeda (tema no legal)
            if not clasificacion['requiere_busqueda']:
                logger.info("üö´ Consulta no legal, redirigiendo profesionalmente...")
                
                respuesta_profesional = """**CONSULTA FUERA DEL √ÅMBITO LEGAL**

COLEPA se especializa exclusivamente en normativa jur√≠dica paraguaya. La consulta planteada no corresponde al √°mbito de aplicaci√≥n del sistema.

**√ÅMBITOS DE COMPETENCIA:**
- Legislaci√≥n civil, penal y procesal
- Normativa laboral y administrativa  
- C√≥digos especializados (aduanero, electoral, sanitario)
- Organizaci√≥n judicial

Para consultas de otra naturaleza, dir√≠jase a los servicios especializados correspondientes."""
                
                tiempo_procesamiento = time.time() - start_time
                actualizar_metricas(False, tiempo_procesamiento, "no_legal")
                
                return ConsultaResponse(
                    respuesta=respuesta_profesional,
                    fuente=None,
                    recomendaciones=None,
                    tiempo_procesamiento=round(tiempo_procesamiento, 2),
                    es_respuesta_oficial=True
                )
        
        # Registrar consulta en sistema de sugerencias para tracking
        sugerencias_manager.registrar_consulta(pregunta_actual)
        
        # ========== CLASIFICACI√ìN Y B√öSQUEDA PREMIUM CON CACHE ==========
        collection_name = clasificar_consulta_con_ia_robusta(pregunta_actual)
        logger.info(f"üìö C√≥digo legal identificado (PREMIUM + CACHE + RETRY): {collection_name}")
        
        # ========== B√öSQUEDA MULTI-M√âTODO CON VALIDACI√ìN Y CACHE ==========
        contexto = None
        if VECTOR_SEARCH_AVAILABLE:
            contexto = buscar_con_manejo_errores(pregunta_actual, collection_name)
        
        # Validar contexto final con est√°ndares premium
        contexto_valido = False
        if contexto and isinstance(contexto, dict) and contexto.get("pageContent"):
            es_valido, score_relevancia = validar_calidad_contexto(contexto, pregunta_actual)
            if es_valido and score_relevancia >= 0.3:  # Umbral premium
                contexto_valido = True
                logger.info(f"üìñ Contexto PREMIUM validado:")
                logger.info(f"   - Ley: {contexto.get('nombre_ley', 'N/A')}")
                logger.info(f"   - Art√≠culo: {contexto.get('numero_articulo', 'N/A')}")
                logger.info(f"   - Score relevancia: {score_relevancia:.2f}")
            else:
                logger.warning(f"‚ùå Contexto no cumple est√°ndares premium (score: {score_relevancia:.2f})")
                contexto = None
        else:
            logger.warning("‚ùå No se encontr√≥ contexto legal para modo premium")
        
        # ========== GENERACI√ìN DE RESPUESTA PREMIUM CON CACHE Y RETRY ==========
        respuesta = generar_respuesta_legal_premium(historial_limitado, contexto)
        
        # ========== PREPARAR RESPUESTA ESTRUCTURADA ==========
        tiempo_procesamiento = time.time() - start_time
        fuente = extraer_fuente_legal(contexto)
        
        # Actualizar m√©tricas del sistema
        codigo_identificado = "desconocido"
        for nombre_codigo, collection in MAPA_COLECCIONES.items():
            if collection == collection_name:
                codigo_identificado = nombre_codigo
                break
        
        articulo_encontrado = contexto.get("numero_articulo") if contexto else None
        actualizar_metricas(contexto_valido, tiempo_procesamiento, codigo_identificado, articulo_encontrado)
        
        response_data = ConsultaResponse(
            respuesta=respuesta,
            fuente=fuente,
            recomendaciones=None,  # Modo premium sin recomendaciones autom√°ticas
            tiempo_procesamiento=round(tiempo_procesamiento, 2),
            es_respuesta_oficial=True
        )
        
        # ========== LOG OPTIMIZADO CON CACHE STATS ==========
        cache_stats = cache_manager.get_stats()
        circuit_stats = circuit_breaker.get_status()
        logger.info(f"‚úÖ Consulta PREMIUM + CACHE + RETRY procesada exitosamente en {tiempo_procesamiento:.2f}s")
        logger.info(f"üéØ Contexto encontrado: {contexto_valido}")
        logger.info(f"üöÄ Cache Hit Rate: {cache_stats['hit_rate_percentage']:.1f}%")
        logger.info(f"üõ°Ô∏è Circuit Breaker: OpenAI {'‚úÖ OK' if circuit_stats['openai_disponible'] else '‚ö†Ô∏è Protegido'}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"‚ùå Error procesando consulta premium con cache y retry: {e}")
        
        # Actualizar m√©tricas de error
        tiempo_procesamiento = time.time() - start_time
        actualizar_metricas(False, tiempo_procesamiento, "error")
        
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Error interno del sistema premium",
                "mensaje": "No fue posible procesar su consulta legal en este momento",
                "recomendacion": "Intente nuevamente en unos momentos",
                "codigo_error": str(e)[:100],
                "timestamp": datetime.now().isoformat(),
                "cache_activo": "‚úÖ Sistema de cache operativo",
                "retry_activo": "‚úÖ Reintentos autom√°ticos activos"
            }
        )

# === MANEJO DE ERRORES ===
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": True,
            "status_code": exc.status_code,
            "detalle": exc.detail,
            "timestamp": datetime.now().isoformat(),
            "mensaje_usuario": "Ha ocurrido un error procesando su consulta legal",
            "version": "3.3.0-PREMIUM-CACHE-RETRY"
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(f"‚ùå Error no controlado en modo premium con cache y retry: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": True,
            "status_code": 500,
            "detalle": "Error interno del servidor premium",
            "timestamp": datetime.now().isoformat(),
            "mensaje_usuario": "El sistema premium est√° experimentando dificultades t√©cnicas",
            "version": "3.3.0-PREMIUM-CACHE-RETRY"
        }
    )

# === PUNTO DE ENTRADA ===
if __name__ == "__main__":
    logger.info("üöÄ Iniciando COLEPA PREMIUM v3.3.0 - Sistema Legal Gubernamental COMPLETO")
    logger.info("üèõÔ∏è Optimizado para Demo Congreso Nacional de Paraguay")
    logger.info("‚ö° Cache de 3 niveles: 70% menos latencia, 60% menos costos OpenAI")
    logger.info("üîÑ Retry Logic: 3 intentos autom√°ticos con backoff exponencial")
    logger.info("üõ°Ô∏è Circuit Breaker: Garant√≠a 0 errores 500 en demo")
    logger.info("üîÆ Sugerencias Inteligentes: Auto-completar consultas legales")
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=int(os.getenv("PORT", 8000)),
        reload=False,  # Deshabilitado en producci√≥n
        log_level="info"
    )
4. C√≥digo Procesal Civil - demandas civiles, da√±os, perjuicios
5. C√≥digo Procesal Penal - denuncias penales, investigaciones
6. C√≥digo Aduanero - aduana, importaci√≥n, exportaci√≥n
7. C√≥digo Electoral - elecciones, votos, candidatos
8. C√≥digo de la Ni√±ez y la Adolescencia - menores, ni√±os
9. C√≥digo de Organizaci√≥n Judicial - tribunales, jueces
10. C√≥digo Sanitario - salud, medicina, hospitales

CONSULTA: "{pregunta[:150]}"

Responde solo el nombre exacto (ej: "C√≥digo Penal")"""

    try:
        response = llamar_openai_con_retry(
            modelo="gpt-3.5-turbo",
            mensajes=[{"role": "user", "content": prompt_clasificacion}],
            max_tokens=20,
            timeout=10
        )
        
        codigo_identificado = response.choices[0].message.content.strip()
        
        # LOG DE TOKENS
        if hasattr(response, 'usage'):
            logger.info(f"üí∞ Clasificaci√≥n - Tokens: {response.usage.total_tokens}")
        
        # Mapear respuesta a colecci√≥n
        if codigo_identificado in MAPA_COLECCIONES:
            collection_name = MAPA_COLECCIONES[codigo_identificado]
            logger.info(f"üéØ IA clasific√≥: {codigo_identificado} ‚Üí {collection_name}")
            # ========== GUARDAR EN CACHE NIVEL 1 ==========
            cache_manager.set_clasificacion(pregunta, collection_name)
            return collection_name
        else:
            # Fuzzy matching para nombres similares
            for codigo_oficial in MAPA_COLECCIONES.keys():
                if any(word in codigo_identificado.lower() for word in codigo_oficial.lower().split()):
                    collection_name = MAPA_COLECCIONES[codigo_oficial]
                    logger.info(f"üéØ IA clasific√≥ (fuzzy): {codigo_identificado} ‚Üí {codigo_oficial}")
                    cache_manager.set_clasificacion(pregunta, collection_name)
                    return collection_name
            
            # Fallback
            logger.warning(f"‚ö†Ô∏è IA devolvi√≥ c√≥digo no reconocido: {codigo_identificado}")
            resultado = clasificar_consulta_inteligente(pregunta)
            cache_manager.set_clasificacion(pregunta, resultado)
            return resultado
            
    except Exception as e:
        logger.error(f"‚ùå Error en clasificaci√≥n con IA: {e}")
        resultado = clasificar_consulta_inteligente(pregunta)
        cache_manager.set_clasificacion(pregunta, resultado)
        return resultado

def truncar_contexto_inteligente(contexto: str, max_tokens: int = MAX_TOKENS_INPUT_CONTEXTO) -> str:
    """
    TRUNCADO INTELIGENTE PROFESIONAL para contextos legales
    CON DETECCI√ìN AUTOM√ÅTICA DE ART√çCULOS LARGOS ESPEC√çFICOS
    """
    if not contexto:
        return ""
    
    # ========== DETECCI√ìN AUTOM√ÅTICA DE ART√çCULOS ESPEC√çFICOS LARGOS ==========
    contexto_lower = contexto.lower()
    
    # Detectar si es consulta por art√≠culo espec√≠fico
    es_articulo_especifico = bool(re.search(r'art[√≠i]culo\s+\d+', contexto_lower))
    
    # Si es art√≠culo espec√≠fico Y es largo, aumentar l√≠mites autom√°ticamente
    if es_articulo_especifico and len(contexto) > 2000:
        max_tokens = 1200  # L√≠mite especial para art√≠culos largos espec√≠ficos
        logger.info(f"üéØ ART√çCULO ESPEC√çFICO LARGO detectado - L√≠mite aumentado a {max_tokens} tokens")
    
    # Estimaci√≥n: 1 token ‚âà 4 caracteres en espa√±ol (conservador)
    max_chars_base = max_tokens * 4
    
    # Si el contexto ya es peque√±o, devolverlo completo
    if len(contexto) <= max_chars_base:
        logger.info(f"üìÑ Contexto completo preservado: {len(contexto)} chars")
        return contexto
    
    # ========== AN√ÅLISIS DE CONTENIDO LEGAL ==========
    contexto_lower = contexto.lower()
    
    # Detectar si es un solo art√≠culo largo vs m√∫ltiples art√≠culos
    patrones_articulos = [
        r'art[√≠i]culo\s+\d+',
        r'art\.\s*\d+',
        r'art√≠culo\s+\d+',
        r'articulo\s+\d+'
    ]
    
    articulos_encontrados = []
    for patron in patrones_articulos:
        matches = re.finditer(patron, contexto_lower)
        for match in matches:
            articulos_encontrados.append(match.start())
    
    es_articulo_unico = len(set(articulos_encontrados)) <= 1
    
    # ========== ESTRATEGIA 1: ART√çCULO √öNICO LARGO ==========
    if es_articulo_unico and len(contexto) <= max_chars_base * 3:  # Cambiado de 2x a 3x
        logger.info(f"üìã Art√≠culo √∫nico detectado - Aumentando l√≠mite para preservar completo")
        # Para art√≠culo √∫nico, permitir hasta 3x el l√≠mite (mejor calidad legal)
        return contexto
    
    # ========== ESTRATEGIA 2: M√öLTIPLES ART√çCULOS - PRIORIZACI√ìN INTELIGENTE ==========
    lineas = contexto.split('\n')
    
    # Clasificar l√≠neas por importancia jur√≠dica
    lineas_criticas = []      # Encabezados de art√≠culos, disposiciones principales
    lineas_importantes = []   # Contenido sustantivo, sanciones, procedimientos
    lineas_contextuales = []  # Definiciones, referencias, aclaraciones
    lineas_secundarias = []   # Texto de relleno, conectores
    
    for linea in lineas:
        linea_lower = linea.lower().strip()
        
        if not linea_lower:
            continue
            
        # CR√çTICAS: Encabezados de art√≠culos y disposiciones principales
        if re.search(r'art[√≠i]culo\s+\d+|^art\.\s*\d+|^cap√≠tulo|^t√≠tulo|^libro', linea_lower):
            lineas_criticas.append(linea)
        
        # IMPORTANTES: Contenido sustantivo legal
        elif any(keyword in linea_lower for keyword in [
            'establece', 'dispone', 'determina', 'ordena', 'proh√≠be', 'permite',
            'sanciona', 'multa', 'pena', 'prisi√≥n', 'reclusi√≥n',
            'procedimiento', 'tr√°mite', 'requisito', 'obligaci√≥n', 'derecho',
            'responsabilidad', 'competencia', 'jurisdicci√≥n'
        ]):
            lineas_importantes.append(linea)
        
        # CONTEXTUALES: Definiciones y referencias
        elif any(keyword in linea_lower for keyword in [
            'entiende', 'considera', 'define', 'significa',
            'presente ley', 'presente c√≥digo', 'reglament',
            'excepci√≥n', 'caso', 'cuando', 'siempre que'
        ]):
            lineas_contextuales.append(linea)
        
        # SECUNDARIAS: Resto del contenido
        else:
            lineas_secundarias.append(linea)
    
    # ========== RECONSTRUCCI√ìN PRIORITARIA ==========
    texto_final = ""
    
    # 1. Siempre incluir l√≠neas cr√≠ticas (encabezados de art√≠culos)
    for linea in lineas_criticas:
        if len(texto_final) + len(linea) + 1 <= max_chars_base * 1.5:  # 50% m√°s para cr√≠ticas
            texto_final += linea + '\n'
        else:
            break
    
    # 2. Agregar l√≠neas importantes hasta el l√≠mite
    chars_restantes = max_chars_base - len(texto_final)
    for linea in lineas_importantes:
        if len(texto_final) + len(linea) + 1 <= max_chars_base:
            texto_final += linea + '\n'
        else:
            break
    
    # 3. Si hay espacio, agregar contextuales
    for linea in lineas_contextuales:
        if len(texto_final) + len(linea) + 1 <= max_chars_base:
            texto_final += linea + '\n'
        else:
            break
    
    # 4. Completar con secundarias si hay espacio
    for linea in lineas_secundarias:
        if len(texto_final) + len(linea) + 1 <= max_chars_base:
            texto_final += linea + '\n'
        else:
            break
    
    # ========== VERIFICACI√ìN DE COHERENCIA JUR√çDICA ==========
    texto_final = texto_final.strip()
    
    # Asegurar que no termina en medio de una oraci√≥n cr√≠tica
    if texto_final and not texto_final.endswith('.'):
        # Buscar el √∫ltimo punto antes del final
        ultimo_punto = texto_final.rfind('.')
        if ultimo_punto > len(texto_final) * 0.8:  # Si est√° en el √∫ltimo 20%
            texto_final = texto_final[:ultimo_punto + 1]
    
    # ========== INDICADOR DE TRUNCADO PROFESIONAL ==========
    if len(contexto) > len(texto_final):
        # Verificar si se perdi√≥ informaci√≥n cr√≠tica
        articulos_originales = len(re.findall(r'art[√≠i]culo\s+\d+', contexto.lower()))
        articulos_finales = len(re.findall(r'art[√≠i]culo\s+\d+', texto_final.lower()))
        
        if articulos_finales < articulos_originales:
            texto_final += f"\n\n[NOTA LEGAL: Contexto optimizado - {articulos_finales} de {articulos_originales} art√≠culos incluidos]"
        else:
            texto_final += "\n\n[NOTA LEGAL: Contenido optimizado preservando disposiciones principales]"
    
    # ========== LOGGING PROFESIONAL ==========
    tokens_estimados = len(texto_final) // 4
    porcentaje_preservado = (len(texto_final) / len(contexto)) * 100
    
    logger.info(f"üìã Truncado inteligente aplicado:")
    logger.info(f"   üìè Original: {len(contexto)} chars ‚Üí Final: {len(texto_final)} chars")
    logger.info(f"   üéØ Preservado: {porcentaje_preservado:.1f}% del contenido original")
    logger.info(f"   üí∞ Tokens estimados: {tokens_estimados}/{max_tokens}")
    logger.info(f"   üìö Estrategia: {'Art√≠culo √∫nico' if es_articulo_unico else 'M√∫ltiples art√≠culos priorizados'}")
    
    return texto_final

# ========== FUNCI√ìN GENERACI√ìN DE RESPUESTA CON CACHE NIVEL 3 ==========
def generar_respuesta_legal_premium(historial: List[MensajeChat], contexto: Optional[Dict] = None) -> str:
    """
    Generaci√≥n de respuesta legal PREMIUM con CIRCUIT BREAKER y CACHE INTELIGENTE
    GARANTIZA respuesta siempre, nunca error 500 en demo
    """
    # ========== CACHE NIVEL 3: VERIFICAR RESPUESTA COMPLETA EN CACHE ==========
    respuesta_cached = cache_manager.get_respuesta(historial, contexto)
    if respuesta_cached:
        logger.info("üöÄ CACHE HIT - Respuesta completa recuperada del cache, evitando llamada costosa a OpenAI")
        return respuesta_cached
    
    # ========== CIRCUIT BREAKER: VERIFICAR ESTADO DE SERVICIOS ==========
    circuit_breaker.reset_if_needed()
    
    if not OPENAI_AVAILABLE or not openai_client or circuit_breaker.should_skip_openai():
        logger.warning("‚ö†Ô∏è OpenAI no disponible o circuit breaker activo - Usando fallback")
        resultado = generar_respuesta_con_contexto(historial[-1].content, contexto)
        cache_manager.set_respuesta(historial, contexto, resultado)
        return resultado
    
    try:
        pregunta_actual = historial[-1].content
        
        # Validar contexto antes de procesar
        if contexto:
            es_valido, score_relevancia = validar_calidad_contexto(contexto, pregunta_actual)
            if not es_valido:
                logger.warning(f"‚ö†Ô∏è Contexto no v√°lido (score: {score_relevancia:.2f}), generando respuesta sin contexto")
                contexto = None
        
        # Preparar mensajes para OpenAI con L√çMITES ESTRICTOS
        mensajes = [{"role": "system", "content": INSTRUCCION_SISTEMA_LEGAL_PREMIUM}]
        
        # Construcci√≥n del prompt con CONTROL DE TOKENS
        if contexto and contexto.get("pageContent"):
            ley = contexto.get('nombre_ley', 'Legislaci√≥n paraguaya')
            articulo = contexto.get('numero_articulo', 'N/A')
            contenido_legal = contexto.get('pageContent', '')
            
            # TRUNCAR CONTEXTO INTELIGENTEMENTE
            contenido_truncado = truncar_contexto_inteligente(contenido_legal)
            
            # PROMPT COMPACTO OPTIMIZADO
            prompt_profesional = f"""CONSULTA: {pregunta_actual[:200]}

NORMA: {ley} - Art. {articulo}
TEXTO: {contenido_truncado}

Responda en formato estructurado."""
            
            mensajes.append({"role": "user", "content": prompt_profesional})
            logger.info(f"üìñ Prompt generado - Chars: {len(prompt_profesional)}")
        else:
            # Sin contexto - RESPUESTA ULTRA COMPACTA
            prompt_sin_contexto = f"""CONSULTA: {pregunta_actual[:150]}

Sin normativa espec√≠fica encontrada. Respuesta profesional breve."""
            
            mensajes.append({"role": "user", "content": prompt_sin_contexto})
            logger.info("üìù Prompt sin contexto - Modo compacto")
        
        # ========== CIRCUIT BREAKER: INTENTAR GPT-4 PRIMERO ==========
        modelo_a_usar = "gpt-4-turbo-preview"
        if circuit_breaker.should_skip_gpt4():
            modelo_a_usar = "gpt-3.5-turbo"
            logger.info("üîÑ Circuit breaker: Usando GPT-3.5 en lugar de GPT-4")
        
        # Llamada a OpenAI con CIRCUIT BREAKER Y RETRY
        response = llamar_openai_con_retry(
            modelo=modelo_a_usar,
            mensajes=mensajes,
            max_tokens=MAX_TOKENS_RESPUESTA,
            timeout=25
        )
        
        respuesta = response.choices[0].message.content
        
        # LOG DE TOKENS UTILIZADOS
        if hasattr(response, 'usage'):
            tokens_input = response.usage.prompt_tokens
            tokens_output = response.usage.completion_tokens
            tokens_total = response.usage.total_tokens
            logger.info(f"üí∞ Tokens utilizados - Input: {tokens_input}, Output: {tokens_output}, Total: {tokens_total}")
        
        # ========== GUARDAR EN CACHE NIVEL 3 ==========
        cache_manager.set_respuesta(historial, contexto, respuesta)
        
        logger.info(f"‚úÖ Respuesta premium generada con {modelo_a_usar}")
        return respuesta
        
    except Exception as e:
        logger.error(f"‚ùå Error con OpenAI en modo premium: {e}")
        
        # ========== CIRCUIT BREAKER: REGISTRAR FALLO Y USAR FALLBACKS ==========
        if "gpt-4" in modelo_a_usar:
            circuit_breaker.record_gpt4_failure()
            # Intentar GPT-3.5 como fallback
            if not circuit_breaker.should_skip_gpt35():
                try:
                    logger.info("üîÑ Fallback: Intentando GPT-3.5...")
                    response = llamar_openai_con_retry(
                        modelo="gpt-3.5-turbo",
                        mensajes=mensajes,
                        max_tokens=MAX_TOKENS_RESPUESTA,
                        timeout=15
                    )
                    resultado = response.choices[0].message.content
                    cache_manager.set_respuesta(historial, contexto, resultado)
                    logger.info("‚úÖ Fallback GPT-3.5 exitoso")
                    return resultado
                except Exception as e2:
                    logger.error(f"‚ùå Fallback GPT-3.5 tambi√©n fall√≥: {e2}")
                    circuit_breaker.record_gpt35_failure()
        else:
            circuit_breaker.record_gpt35_failure()
        
        circuit_breaker.record_openai_failure()
        
        # ========== FALLBACK FINAL: TEMPLATE DE EMERGENCIA ==========
        if contexto:
            resultado = generar_respuesta_con_contexto(historial[-1].content, contexto)
        else:
            logger.warning("üÜò Usando template de emergencia para garantizar respuesta")
            resultado = generar_respuesta_template_emergencia(historial[-1].content)
        
        cache_manager.set_respuesta(historial, contexto, resultado)
        return resultado

def generar_respuesta_con_contexto(pregunta: str, contexto: Optional[Dict] = None) -> str:
    """
    Respuesta directa PREMIUM usando el contexto de Qdrant
    """
    if contexto and contexto.get("pageContent"):
        ley = contexto.get('nombre_ley', 'Legislaci√≥n paraguaya')
        articulo = contexto.get('numero_articulo', 'N/A')
        contenido = contexto.get('pageContent', '')
        
        # Formato profesional estructurado
        response = f"""**DISPOSICI√ìN LEGAL**
{ley}, Art√≠culo {articulo}

**FUNDAMENTO NORMATIVO**
{contenido}

**APLICACI√ìN JUR√çDICA**
La disposici√≥n citada responde directamente a la consulta planteada sobre "{pregunta}".

---
*Fuente: {ley}, Art√≠culo {articulo}*
*Para asesoramiento espec√≠fico, consulte con profesional del derecho especializado.*"""
        
        logger.info(f"‚úÖ Respuesta premium generada con contexto: {ley} Art. {articulo}")
        return response
    else:
        return f"""**CONSULTA LEGAL - INFORMACI√ìN NO DISPONIBLE**

No se encontr√≥ disposici√≥n normativa espec√≠fica aplicable a: "{pregunta}"

**RECOMENDACIONES PROCESALES:**
1. **Reformule la consulta** con mayor especificidad t√©cnica
2. **Especifique el cuerpo normativo** de su inter√©s (C√≥digo Civil, Penal, etc.)
3. **Indique n√∫mero de art√≠culo** si conoce la disposici√≥n espec√≠fica

**√ÅREAS DE CONSULTA DISPONIBLES:**
- Normativa civil (familia, contratos, propiedad)
- Normativa penal (delitos, procedimientos)
- Normativa laboral (relaciones de trabajo)
- Normativa procesal (procedimientos judiciales)

*Para consultas espec√≠ficas sobre casos particulares, dir√≠jase a profesional del derecho competente.*"""

def extraer_fuente_legal(contexto: Optional[Dict]) -> Optional[FuenteLegal]:
    """
    Extrae informaci√≥n de la fuente legal del contexto
    """
    if not contexto:
        return None
    
    return FuenteLegal(
        ley=contexto.get("nombre_ley", "No especificada"),
        articulo_numero=str(contexto.get("numero_articulo", "N/A")),
        libro=contexto.get("libro"),
        titulo=contexto.get("titulo")
    )

def actualizar_metricas(tiene_contexto: bool, tiempo_procesamiento: float, codigo: str, articulo: Optional[str] = None):
    """
    Actualiza m√©tricas del sistema para monitoreo en tiempo real
    """
    global metricas_sistema
    
    metricas_sistema["consultas_procesadas"] += 1
    if tiene_contexto:
        metricas_sistema["contextos_encontrados"] += 1
    
    # Actualizar tiempo promedio
    total_consultas = metricas_sistema["consultas_procesadas"]
    tiempo_anterior = metricas_sistema["tiempo_promedio"]
    metricas_sistema["tiempo_promedio"] = ((tiempo_anterior * (total_consultas - 1)) + tiempo_procesamiento) / total_consultas
    
    metricas_sistema["ultima_actualizacion"] = datetime.now()
    
    logger.info(f"üìä M√©tricas actualizadas - Consultas: {total_consultas}, Contextos: {metricas_sistema['contextos_encontrados']}")

# === MIDDLEWARE ===
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    client_ip = request.client.host
    logger.info(f"üì• {request.method} {request.url.path} - IP: {client_ip}")
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(f"üì§ {response.status_code} - {process_time:.2f}s")
    
    return response

# === ENDPOINTS ===
@app.get("/", response_model=StatusResponse)
async def sistema_status():
    """Estado del sistema COLEPA"""
    return StatusResponse(
        status="‚úÖ Sistema COLEPA Premium Operativo con Cache Inteligente",
        timestamp=datetime.now(),
        version="3.3.0-PREMIUM-CACHE",
        servicios={
            "openai": "disponible" if OPENAI_AVAILABLE else "no disponible",
            "busqueda_vectorial": "disponible" if VECTOR_SEARCH_AVAILABLE else "modo_demo",
            "base_legal": "legislaci√≥n paraguaya completa",
            "modo": "PREMIUM - Demo Congreso Nacional",
            "cache_inteligente": "‚úÖ activo 3 niveles",
            "retry_logic": "‚úÖ activo 3 intentos",
            "circuit_breaker": "‚úÖ protecci√≥n demo",
            "sugerencias_inteligentes": "‚úÖ auto-completar activo"
        },
        colecciones_disponibles=len(MAPA_COLECCIONES)
    )

@app.get("/api/health")
async def health_check():
    """Verificaci√≥n de salud detallada"""
    health_status = {
        "sistema": "operativo",
        "timestamp": datetime.now().isoformat(),
        "version": "3.3.0-PREMIUM-CACHE-RETRY",
        "modo": "Demo Congreso Nacional",
        "servicios": {
            "openai": "‚ùå no disponible",
            "qdrant": "‚ùå no disponible" if not VECTOR_SEARCH_AVAILABLE else "‚úÖ operativo",
            "base_legal": "‚úÖ cargada",
            "validacion_contexto": "‚úÖ activa",
            "busqueda_multi_metodo": "‚úÖ activa",
            "cache_inteligente": "‚úÖ operativo 3 niveles",
            "retry_logic": "‚úÖ activo",
            "circuit_breaker": "‚úÖ protecci√≥n activa",
            "sugerencias_inteligentes": "‚úÖ auto-completar operativo"
        },
        "cache_stats": cache_manager.get_stats(),
        "circuit_breaker_stats": circuit_breaker.get_status()
    }
    
    if OPENAI_AVAILABLE and openai_client:
        try:
            # Test m√≠nimo de OpenAI CON RETRY
            response = llamar_openai_con_retry(
                modelo="gpt-3.5-turbo",
                mensajes=[{"role": "user", "content": "test"}],
                max_tokens=1,
                timeout=10
            )
            health_status["servicios"]["openai"] = "‚úÖ operativo"
        except Exception as e:
            health_status["servicios"]["openai"] = f"‚ùå error: {str(e)[:50]}"
    
    return health_status

@app.get("/api/codigos")
async def listar_codigos_legales():
    """Lista todos los c√≥digos legales disponibles"""
    return {
        "codigos_disponibles": list(MAPA_COLECCIONES.keys()),
        "total_codigos": len(MAPA_COLECCIONES),
        "descripcion": "C√≥digos legales completos de la Rep√∫blica del Paraguay",
        "ultima_actualizacion": "2024",
        "cobertura": "Legislaci√≥n nacional vigente",
        "modo": "PREMIUM - Optimizado para profesionales del derecho",
        "cache_optimizado": "‚úÖ Cache inteligente de 3 niveles activo",
        "retry_logic": "‚úÖ Reintentos autom√°ticos activos",
        "circuit_breaker": "‚úÖ Protecci√≥n de fallos activa",
        "sugerencias_inteligentes": "‚úÖ Auto-completar consultas activo"
    }

# ========== NUEVO ENDPOINT: M√âTRICAS CON CACHE ==========
@app.get("/api/metricas")
async def obtener_metricas():
    """M√©tricas del sistema con tracking de tokens y estad√≠sticas de cache"""
    global metricas_sistema
    
    # Calcular porcentaje de √©xito
    total_consultas = metricas_sistema["consultas_procesadas"]
    contextos_encontrados = metricas_sistema["contextos_encontrados"]
    
    porcentaje_exito = (contextos_encontrados / total_consultas * 100) if total_consultas > 0 else 0
    
    # Obtener estad√≠sticas del cache
    cache_stats = cache_manager.get_stats()
    circuit_stats = circuit_breaker.get_status()
    
    return {
        "estado_sistema": "‚úÖ PREMIUM OPERATIVO CON CACHE + RETRY",
        "version": "3.3.0-PREMIUM-CACHE-RETRY-OPTIMIZADO",
        "timestamp": datetime.now().isoformat(),
        "metricas": {
            "total_consultas_procesadas": total_consultas,
            "contextos_legales_encontrados": contextos_encontrados,
            "porcentaje_exito": round(porcentaje_exito, 1),
            "tiempo_promedio_respuesta": round(metricas_sistema["tiempo_promedio"], 2),
            "ultima_actualizacion": metricas_sistema["ultima_actualizacion"].isoformat()
        },
        "cache_performance": cache_stats,
        "circuit_breaker_status": circuit_stats,
        "sugerencias_performance": sugerencias_stats,
        "optimizacion_tokens": {
            "max_tokens_respuesta": MAX_TOKENS_RESPUESTA,
            "max_tokens_contexto": MAX_TOKENS_INPUT_CONTEXTO,
            "max_tokens_sistema": MAX_TOKENS_SISTEMA,
            "modelo_clasificacion": "gpt-3.5-turbo (econ√≥mico)",
            "modelo_respuesta": "gpt-4-turbo-preview (calidad)"
        },
        "configuracion": {
            "validacion_contexto_activa": True,
            "busqueda_multi_metodo": True,
            "formato_profesional": True,
            "control_costos_activo": True,
            "cache_inteligente_activo": True,
            "retry_logic_activo": True,
            "circuit_breaker_activo": True,
            "sugerencias_inteligentes_activo": True,
            "optimizado_para": "Congreso Nacional de Paraguay"
        }
    }

# ========== NUEVO ENDPOINT: ESTAD√çSTICAS DEL CACHE ==========
@app.get("/api/cache-stats")
async def obtener_estadisticas_cache():
    """Estad√≠sticas detalladas del cache para monitoreo"""
    return {
        "cache_status": "‚úÖ Operativo",
        "timestamp": datetime.now().isoformat(),
        "estadisticas": cache_manager.get_stats(),
        "beneficios_estimados": {
            "reduccion_latencia": f"{cache_manager.get_stats()['hit_rate_percentage']:.1f}% de consultas instant√°neas",
            "ahorro_openai_calls": f"~{cache_manager.hits_clasificaciones + cache_manager.hits_respuestas} llamadas evitadas",
            "ahorro_qdrant_calls": f"~{cache_manager.hits_contextos} b√∫squedas evitadas"
        }
    }

# ========== NUEVO ENDPOINT: CONSULTA CON INDICADORES EN TIEMPO REAL ==========
@app.get("/api/consulta-stream/{consulta_id}")
async def stream_procesamiento_consulta(consulta_id: str):
    """
    Endpoint para streaming de indicadores de procesamiento en tiempo real
    Usado por el frontend para mostrar progreso durante consultas largas
    """
    logger.info(f"üé¨ Iniciando stream de procesamiento para consulta: {consulta_id}")
    
    # Headers para Server-Sent Events
    headers = {
        "Content-Type": "text/plain; charset=utf-8",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "Access-Control-Allow-Origin": "*",
        "Access-Control-Allow-Headers": "*"
    }
    
    # Simular progreso (en implementaci√≥n real, esto vendr√≠a de la consulta actual)
    async def evento_demo():
        eventos = [
            ("inicio", "üèõÔ∏è COLEPA iniciando an√°lisis legal...", 10),
            ("clasificacion", "üß† Analizando tipo de consulta jur√≠dica...", 25), 
            ("codigo", "üìö Identificando c√≥digo legal aplicable...", 45),
            ("busqueda", "üîç Buscando en base de datos legal paraguaya...", 65),
            ("validacion", "‚öñÔ∏è Validando relevancia jur√≠dica...", 80),
            ("generacion", "üìù Generando respuesta profesional...", 95),
            ("completado", "‚úÖ Consulta legal procesada exitosamente", 100)
        ]
        
        for tipo, mensaje, progreso in eventos:
            data = {
                "consulta_id": consulta_id,
                "tipo": tipo,
                "mensaje": mensaje, 
                "progreso": progreso,
                "timestamp": datetime.now().isoformat()
            }
            yield f"data: {json.dumps(data)# COLEPA - Asistente Legal Gubernamental
# Backend FastAPI Mejorado para Consultas Legales Oficiales - VERSI√ìN PREMIUM v3.3.0 CON CACHE

import os
import re
import time
import logging
import hashlib
import threading
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Tuple

from fastapi import FastAPI, HTTPException, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
import uvicorn
import json
import asyncio

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Verificar y configurar OpenAI
try:
    from openai import OpenAI
    from dotenv import load_dotenv
    
    load_dotenv()
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    OPENAI_AVAILABLE = True
    logger.info("‚úÖ OpenAI configurado correctamente")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è OpenAI no disponible: {e}")
    OPENAI_AVAILABLE = False
    openai_client = None

# Importaciones locales con fallback
try:
    from app.vector_search import buscar_articulo_relevante, buscar_articulo_por_numero
    from app.prompt_builder import construir_prompt
    VECTOR_SEARCH_AVAILABLE = True
    logger.info("‚úÖ M√≥dulos de b√∫squeda vectorial cargados")
except ImportError:
    logger.warning("‚ö†Ô∏è M√≥dulos de b√∫squeda no encontrados, usando funciones mock")
    VECTOR_SEARCH_AVAILABLE = False
    
    def buscar_articulo_relevante(query_vector, collection_name):
        return {
            "pageContent": "Contenido de ejemplo del art√≠culo", 
            "nombre_ley": "C√≥digo Civil", 
            "numero_articulo": "123"
        }
    
    def buscar_articulo_por_numero(numero, collection_name):
        return {
            "pageContent": f"Contenido del art√≠culo {numero}", 
            "nombre_ley": "C√≥digo Civil", 
            "numero_articulo": str(numero)
        }
    
    def construir_prompt(contexto_legal, pregunta_usuario):
        return f"Contexto Legal: {contexto_legal}\n\nPregunta del Usuario: {pregunta_usuario}"

# ========== NUEVO: CLASIFICADOR INTELIGENTE ==========
try:
    from app.clasificador_inteligente import clasificar_y_procesar
    CLASIFICADOR_AVAILABLE = True
    logger.info("‚úÖ Clasificador inteligente cargado")
except ImportError:
    logger.warning("‚ö†Ô∏è Clasificador no encontrado, modo b√°sico")
    CLASIFICADOR_AVAILABLE = False
    
    def clasificar_y_procesar(texto):
        return {
            'tipo_consulta': 'consulta_legal',
            'respuesta_directa': None,
            'requiere_busqueda': True,
            'es_conversacional': False
        }

# ========== NUEVO: SISTEMA DE CACHE INTELIGENTE ==========
class CacheManager:
    """
    Sistema de cache h√≠brido de 3 niveles para optimizar velocidad y costos
    Nivel 1: Clasificaciones (TTL: 1h)
    Nivel 2: Contextos legales (TTL: 24h) 
    Nivel 3: Respuestas completas (TTL: 6h)
    """
    
    def __init__(self, max_memory_mb: int = 100):
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        
        # Cache Level 1: Clasificaciones de c√≥digo legal
        self.cache_clasificaciones = {}  # hash -> (resultado, timestamp)
        self.ttl_clasificaciones = 3600  # 1 hora
        
        # Cache Level 2: Contextos legales de Qdrant
        self.cache_contextos = {}  # hash -> (contexto_dict, timestamp)
        self.ttl_contextos = 86400  # 24 horas
        
        # Cache Level 3: Respuestas completas
        self.cache_respuestas = {}  # hash -> (respuesta_str, timestamp)
        self.ttl_respuestas = 21600  # 6 horas
        
        # M√©tricas del cache
        self.hits_clasificaciones = 0
        self.hits_contextos = 0
        self.hits_respuestas = 0
        self.misses_total = 0
        
        # Thread para limpieza autom√°tica
        self.cleanup_lock = threading.RLock()
        self.start_cleanup_thread()
        
        logger.info(f"üöÄ CacheManager inicializado - L√≠mite: {max_memory_mb}MB")
    
    def _normalize_query(self, text: str) -> str:
        """Normaliza consultas para generar hashes consistentes"""
        if not text:
            return ""
        
        # Convertir a min√∫sculas y limpiar
        normalized = text.lower().strip()
        
        # Remover caracteres especiales pero mantener espacios
        normalized = re.sub(r'[^\w\s]', ' ', normalized)
        
        # Normalizar espacios m√∫ltiples
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # Sin√≥nimos comunes para mejorar hit rate
        synonyms = {
            'articulo': 'art√≠culo',
            'codigo': 'c√≥digo',
            'divorcio': 'divorcio',
            'matrimonio': 'matrimonio',
            'trabajo': 'laboral',
            'empleo': 'laboral',
            'delito': 'penal',
            'crimen': 'penal'
        }
        
        for original, replacement in synonyms.items():
            normalized = normalized.replace(original, replacement)
        
        return normalized.strip()
    
    def _generate_hash(self, *args) -> str:
        """Genera hash √∫nico para m√∫ltiples argumentos"""
        content = "|".join(str(arg) for arg in args if arg is not None)
        return hashlib.md5(content.encode()).hexdigest()
    
    def _is_expired(self, timestamp: float, ttl: int) -> bool:
        """Verifica si una entrada del cache ha expirado"""
        return time.time() - timestamp > ttl
    
    def _estimate_memory_usage(self) -> int:
        """Estima el uso de memoria actual del cache"""
        total_items = (
            len(self.cache_clasificaciones) + 
            len(self.cache_contextos) + 
            len(self.cache_respuestas)
        )
        # Estimaci√≥n: ~1KB promedio por entrada
        return total_items * 1024
    
    def _cleanup_expired(self):
        """Limpia entradas expiradas de todos los niveles"""
        with self.cleanup_lock:
            current_time = time.time()
            
            # Limpiar clasificaciones
            expired_keys = [
                k for k, (_, timestamp) in self.cache_clasificaciones.items()
                if current_time - timestamp > self.ttl_clasificaciones
            ]
            for key in expired_keys:
                del self.cache_clasificaciones[key]
            
            # Limpiar contextos
            expired_keys = [
                k for k, (_, timestamp) in self.cache_contextos.items()
                if current_time - timestamp > self.ttl_contextos
            ]
            for key in expired_keys:
                del self.cache_contextos[key]
            
            # Limpiar respuestas
            expired_keys = [
                k for k, (_, timestamp) in self.cache_respuestas.items()
                if current_time - timestamp > self.ttl_respuestas
            ]
            for key in expired_keys:
                del self.cache_respuestas[key]
            
            if expired_keys:
                logger.info(f"üßπ Cache cleanup: {len(expired_keys)} entradas expiradas eliminadas")
    
    def _evict_lru_if_needed(self):
        """Elimina entradas LRU si se excede el l√≠mite de memoria"""
        if self._estimate_memory_usage() > self.max_memory_bytes:
            # Implementaci√≥n simple LRU: eliminar 10% m√°s antiguas
            all_entries = []
            
            for k, (v, t) in self.cache_clasificaciones.items():
                all_entries.append((t, 'clasificaciones', k))
            for k, (v, t) in self.cache_contextos.items():
                all_entries.append((t, 'contextos', k))
            for k, (v, t) in self.cache_respuestas.items():
                all_entries.append((t, 'respuestas', k))
            
            # Ordenar por timestamp (m√°s antiguas primero)
            all_entries.sort(key=lambda x: x[0])
            
            # Eliminar 10% m√°s antiguas
            to_evict = max(1, len(all_entries) // 10)
            
            for _, cache_type, key in all_entries[:to_evict]:
                if cache_type == 'clasificaciones' and key in self.cache_clasificaciones:
                    del self.cache_clasificaciones[key]
                elif cache_type == 'contextos' and key in self.cache_contextos:
                    del self.cache_contextos[key]
                elif cache_type == 'respuestas' and key in self.cache_respuestas:
                    del self.cache_respuestas[key]
            
            logger.info(f"üíæ Cache LRU eviction: {to_evict} entradas eliminadas")
    
    def start_cleanup_thread(self):
        """Inicia thread de limpieza autom√°tica cada 5 minutos"""
        def cleanup_worker():
            while True:
                try:
                    time.sleep(300)  # 5 minutos
                    self._cleanup_expired()
                    self._evict_lru_if_needed()
                except Exception as e:
                    logger.error(f"‚ùå Error en cleanup autom√°tico: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()
        logger.info("üßπ Thread de limpieza autom√°tica iniciado")
    
    # ========== M√âTODOS DE CACHE NIVEL 1: CLASIFICACIONES ==========
    def get_clasificacion(self, pregunta: str) -> Optional[str]:
        """Obtiene clasificaci√≥n del cache"""
        normalized_query = self._normalize_query(pregunta)
        cache_key = self._generate_hash(normalized_query)
        
        if cache_key in self.cache_clasificaciones:
            resultado, timestamp = self.cache_clasificaciones[cache_key]
            if not self._is_expired(timestamp, self.ttl_clasificaciones):
                self.hits_clasificaciones += 1
                logger.info(f"üéØ Cache HIT - Clasificaci√≥n: {resultado}")
                return resultado
            else:
                del self.cache_clasificaciones[cache_key]
        
        self.misses_total += 1
        return None
    
    def set_clasificacion(self, pregunta: str, resultado: str):
        """Guarda clasificaci√≥n en cache"""
        normalized_query = self._normalize_query(pregunta)
        cache_key = self._generate_hash(normalized_query)
        
        self.cache_clasificaciones[cache_key] = (resultado, time.time())
        logger.info(f"üíæ Cache SET - Clasificaci√≥n: {resultado}")
    
    # ========== M√âTODOS DE CACHE NIVEL 2: CONTEXTOS ==========
    def get_contexto(self, pregunta: str, collection_name: str) -> Optional[Dict]:
        """Obtiene contexto del cache"""
        normalized_query = self._normalize_query(pregunta)
        cache_key = self._generate_hash(normalized_query, collection_name)
        
        if cache_key in self.cache_contextos:
            contexto, timestamp = self.cache_contextos[cache_key]
            if not self._is_expired(timestamp, self.ttl_contextos):
                self.hits_contextos += 1
                logger.info(f"üìñ Cache HIT - Contexto: {contexto.get('nombre_ley', 'N/A')} Art. {contexto.get('numero_articulo', 'N/A')}")
                return contexto
            else:
                del self.cache_contextos[cache_key]
        
        self.misses_total += 1
        return None
    
    def set_contexto(self, pregunta: str, collection_name: str, contexto: Dict):
        """Guarda contexto en cache"""
        normalized_query = self._normalize_query(pregunta)
        cache_key = self._generate_hash(normalized_query, collection_name)
        
        self.cache_contextos[cache_key] = (contexto, time.time())
        ley = contexto.get('nombre_ley', 'N/A')
        art = contexto.get('numero_articulo', 'N/A')
        logger.info(f"üíæ Cache SET - Contexto: {ley} Art. {art}")
    
    # ========== M√âTODOS DE CACHE NIVEL 3: RESPUESTAS ==========
    def get_respuesta(self, historial: List, contexto: Optional[Dict]) -> Optional[str]:
        """Obtiene respuesta completa del cache"""
        # Generar hash del historial + contexto
        historial_text = " ".join([msg.content for msg in historial[-3:]])  # √öltimos 3 mensajes
        normalized_historial = self._normalize_query(historial_text)
        
        contexto_hash = ""
        if contexto:
            contexto_hash = self._generate_hash(
                contexto.get('nombre_ley', ''),
                contexto.get('numero_articulo', ''),
                contexto.get('pageContent', '')[:200]  # Primeros 200 chars
            )
        
        cache_key = self._generate_hash(normalized_historial, contexto_hash)
        
        if cache_key in self.cache_respuestas:
            respuesta, timestamp = self.cache_respuestas[cache_key]
            if not self._is_expired(timestamp, self.ttl_respuestas):
                self.hits_respuestas += 1
                logger.info(f"üí¨ Cache HIT - Respuesta completa ({len(respuesta)} chars)")
                return respuesta
            else:
                del self.cache_respuestas[cache_key]
        
        self.misses_total += 1
        return None
    
    def set_respuesta(self, historial: List, contexto: Optional[Dict], respuesta: str):
        """Guarda respuesta completa en cache"""
        historial_text = " ".join([msg.content for msg in historial[-3:]])
        normalized_historial = self._normalize_query(historial_text)
        
        contexto_hash = ""
        if contexto:
            contexto_hash = self._generate_hash(
                contexto.get('nombre_ley', ''),
                contexto.get('numero_articulo', ''),
                contexto.get('pageContent', '')[:200]
            )
        
        cache_key = self._generate_hash(normalized_historial, contexto_hash)
        
        self.cache_respuestas[cache_key] = (respuesta, time.time())
        logger.info(f"üíæ Cache SET - Respuesta completa ({len(respuesta)} chars)")
    
    # ========== M√âTRICAS DEL CACHE ==========
    def get_stats(self) -> Dict:
        """Obtiene estad√≠sticas del cache"""
        total_hits = self.hits_clasificaciones + self.hits_contextos + self.hits_respuestas
        total_requests = total_hits + self.misses_total
        hit_rate = (total_hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            "hit_rate_percentage": round(hit_rate, 1),
            "total_hits": total_hits,
            "total_misses": self.misses_total,
            "hits_por_nivel": {
                "clasificaciones": self.hits_clasificaciones,
                "contextos": self.hits_contextos,
                "respuestas": self.hits_respuestas
            },
            "entradas_cache": {
                "clasificaciones": len(self.cache_clasificaciones),
                "contextos": len(self.cache_contextos), 
                "respuestas": len(self.cache_respuestas)
            },
            "memoria_estimada_mb": round(self._estimate_memory_usage() / 1024 / 1024, 2),
            "limite_memoria_mb": round(self.max_memory_bytes / 1024 / 1024, 2)
        }

# ========== NUEVO: CIRCUIT BREAKER PARA DEMO SIN FALLOS ==========
class CircuitBreaker:
    """
    Circuit Breaker para garantizar 0 errores 500 en demo del Congreso Nacional
    Fallbacks: GPT-4 ‚Üí GPT-3.5 ‚Üí Templates ‚Üí B√∫squeda directa
    """
    
    def __init__(self):
        self.gpt4_failures = 0
        self.gpt35_failures = 0
        self.openai_failures = 0
        self.max_failures = 3
        self.reset_time = 300  # 5 minutos
        self.last_failure_time = 0
        
        logger.info("üõ°Ô∏è Circuit Breaker inicializado para demo sin fallos")
    
    def should_skip_gpt4(self) -> bool:
        """Determina si saltar GPT-4 por fallos recientes"""
        return self.gpt4_failures >= self.max_failures
    
    def should_skip_gpt35(self) -> bool:
        """Determina si saltar GPT-3.5 por fallos recientes"""
        return self.gpt35_failures >= self.max_failures
    
    def should_skip_openai(self) -> bool:
        """Determina si saltar completamente OpenAI"""
        return self.openai_failures >= self.max_failures
    
    def record_gpt4_failure(self):
        """Registra fallo de GPT-4"""
        self.gpt4_failures += 1
        self.last_failure_time = time.time()
        logger.warning(f"‚ö†Ô∏è GPT-4 fallo registrado ({self.gpt4_failures}/{self.max_failures})")
    
    def record_gpt35_failure(self):
        """Registra fallo de GPT-3.5"""
        self.gpt35_failures += 1
        self.last_failure_time = time.time()
        logger.warning(f"‚ö†Ô∏è GPT-3.5 fallo registrado ({self.gpt35_failures}/{self.max_failures})")
    
    def record_openai_failure(self):
        """Registra fallo completo de OpenAI"""
        self.openai_failures += 1
        self.last_failure_time = time.time()
        logger.error(f"‚ùå OpenAI fallo registrado ({self.openai_failures}/{self.max_failures})")
    
    def reset_if_needed(self):
        """Resetea contadores si ha pasado tiempo suficiente"""
        if time.time() - self.last_failure_time > self.reset_time:
            if self.gpt4_failures > 0 or self.gpt35_failures > 0 or self.openai_failures > 0:
                logger.info("üîÑ Circuit Breaker reseteado - Reintentando servicios")
            self.gpt4_failures = 0
            self.gpt35_failures = 0
            self.openai_failures = 0
    
    def get_status(self) -> Dict:
        """Estado actual del circuit breaker"""
        return {
            "gpt4_disponible": not self.should_skip_gpt4(),
            "gpt35_disponible": not self.should_skip_gpt35(),
            "openai_disponible": not self.should_skip_openai(),
            "fallos_gpt4": self.gpt4_failures,
            "fallos_gpt35": self.gpt35_failures,
            "fallos_openai": self.openai_failures,
            "tiempo_ultimo_fallo": self.last_failure_time
        }

# ========== TEMPLATES DE EMERGENCIA PARA CIRCUIT BREAKER ==========
TEMPLATES_EMERGENCIA = {
    "matrimonio": """**CONSULTA SOBRE MATRIMONIO - INFORMACI√ìN GENERAL**

**NORMATIVA APLICABLE:** C√≥digo Civil Paraguayo

**DISPOSICIONES PRINCIPALES:**
- Requisitos para contraer matrimonio (mayor√≠a de edad, consentimiento)
- Impedimentos matrimoniales establecidos por ley
- Efectos jur√≠dicos del matrimonio en bienes y familia
- Procedimientos para disoluci√≥n matrimonial

*Consulte C√≥digo Civil, Libro Segundo, T√≠tulo I para disposiciones espec√≠ficas.*
*Para casos particulares, recurra a profesional del derecho especializado.*""",

    "divorcio": """**CONSULTA SOBRE DIVORCIO - INFORMACI√ìN GENERAL**

**NORMATIVA APLICABLE:** C√≥digo Civil Paraguayo

**CAUSALES PRINCIPALES:**
- Divorcio por mutuo consentimiento
- Divorcio contencioso por causales espec√≠ficas
- Procedimientos ante Juzgados de Primera Instancia
- Efectos sobre bienes y tenencia de hijos

*Consulte C√≥digo Civil y C√≥digo Procesal Civil para procedimientos.*
*Requiere asesoramiento legal profesional para casos espec√≠ficos.*""",

    "laboral": """**CONSULTA LABORAL - INFORMACI√ìN GENERAL**

**NORMATIVA APLICABLE:** C√≥digo Laboral Paraguayo

**ASPECTOS PRINCIPALES:**
- Derechos y obligaciones laborales
- Jornada de trabajo y descansos
- Salarios y beneficios sociales
- Procedimientos de despido e indemnizaciones

*Consulte C√≥digo Laboral para disposiciones espec√≠ficas.*
*Para conflictos laborales, acuda a profesional especializado.*""",

    "penal": """**CONSULTA PENAL - INFORMACI√ìN GENERAL**

**NORMATIVA APLICABLE:** C√≥digo Penal Paraguayo

**ASPECTOS PRINCIPALES:**
- Tipificaci√≥n de delitos y faltas
- Sanciones y penas establecidas
- Procedimientos de denuncia
- Derechos del imputado y v√≠ctima

*Para denuncias, acuda a Comisar√≠as o Ministerio P√∫blico.*
*Requiere asesoramiento legal especializado urgente.*"""
}

def generar_respuesta_template_emergencia(pregunta: str) -> str:
    """
    Genera respuesta usando templates cuando fallan todos los servicios IA
    GARANTIZA que siempre haya respuesta en demo
    """
    pregunta_lower = pregunta.lower()
    
    # Detectar tema principal
    if any(palabra in pregunta_lower for palabra in ["matrimonio", "esposo", "esposa", "c√≥nyuge", "casar"]):
        return TEMPLATES_EMERGENCIA["matrimonio"]
    elif any(palabra in pregunta_lower for palabra in ["divorcio", "separaci√≥n", "disoluci√≥n"]):
        return TEMPLATES_EMERGENCIA["divorcio"]
    elif any(palabra in pregunta_lower for palabra in ["trabajo", "empleado", "laboral", "salario", "despido"]):
        return TEMPLATES_EMERGENCIA["laboral"]
    elif any(palabra in pregunta_lower for palabra in ["delito", "penal", "crimen", "denuncia", "violencia"]):
        return TEMPLATES_EMERGENCIA["penal"]
    else:
        # Template gen√©rico para cualquier consulta
        return """**CONSULTA LEGAL - RESPUESTA GENERAL**

**SISTEMA:** COLEPA - Asistente Legal del Congreso Nacional

**INFORMACI√ìN:** El sistema ha identificado su consulta legal pero requiere mayor especificidad para brindar respuesta precisa.

**C√ìDIGOS DISPONIBLES:**
- C√≥digo Civil (familia, matrimonio, contratos)
- C√≥digo Penal (delitos, procedimientos penales)  
- C√≥digo Laboral (relaciones de trabajo)
- C√≥digos Procesales (procedimientos judiciales)

**RECOMENDACI√ìN:** Reformule su consulta especificando:
1. √Årea legal de inter√©s
2. Art√≠culo espec√≠fico (si lo conoce)
3. Situaci√≥n particular a consultar

*Para asesoramiento espec√≠fico, consulte con profesional del derecho.*"""

# ========== SISTEMA DE SUGERENCIAS INTELIGENTES ==========
class SugerenciasManager:
    """
    Sistema de sugerencias inteligentes para auto-completar consultas frecuentes
    Basado en patrones de uso y c√≥digos legales
    """
    
    def __init__(self):
        self.consultas_frecuentes = {}  # query -> contador
        self.sugerencias_por_codigo = {}  # codigo -> [sugerencias]
        self.inicializar_sugerencias_base()
        logger.info("üîÆ SugerenciasManager inicializado")
    
    def inicializar_sugerencias_base(self):
        """Inicializa sugerencias base por c√≥digo legal"""
        self.sugerencias_por_codigo = {
            "C√≥digo Civil": [
                "¬øQu√© dice el art√≠culo 32 sobre matrimonio?",
                "Requisitos para contraer matrimonio en Paraguay",
                "¬øC√≥mo se inicia un proceso de divorcio?",
                "Derechos patrimoniales del matrimonio",
                "¬øQu√© es la sociedad conyugal?",
                "Adopci√≥n de menores seg√∫n el C√≥digo Civil",
                "Herencia y sucesi√≥n en Paraguay",
                "Contratos civiles m√°s comunes"
            ],
            "C√≥digo Penal": [
                "¬øQu√© penas tiene el robo en Paraguay?",
                "Diferencia entre homicidio y asesinato",
                "¬øC√≥mo denunciar violencia dom√©stica?",
                "Delitos contra la propiedad",
                "¬øQu√© hacer en caso de estafa?",
                "Leg√≠tima defensa seg√∫n el C√≥digo Penal",
                "Delitos inform√°ticos en Paraguay",
                "Narcotr√°fico y sus sanciones"
            ],
            "C√≥digo Laboral": [
                "¬øCu√°nto es la indemnizaci√≥n por despido?",
                "Derechos del trabajador en Paraguay",
                "¬øCu√°ntas horas de trabajo son legales?",
                "Vacaciones anuales del empleado",
                "¬øQu√© es el aguinaldo?",
                "Licencia por maternidad",
                "¬øC√≥mo calcular las horas extras?",
                "Seguridad social obligatoria"
            ],
            "C√≥digo Electoral": [
                "Requisitos para ser candidato",
                "¬øC√≥mo funciona el sistema electoral?",
                "Derechos del votante",
                "Proceso de inscripci√≥n electoral",
                "¬øQu√© es el padr√≥n electoral?",
                "Financiamiento de campa√±as",
                "Tribunal Superior de Justicia Electoral",
                "Delitos electorales"
            ],
            "C√≥digo Procesal Civil": [
                "¬øC√≥mo iniciar una demanda civil?",
                "Proceso de cobro de deudas",
                "Medidas cautelares disponibles",
                "¬øQu√© es el embargo preventivo?",
                "Da√±os y perjuicios por accidente",
                "Proceso de ejecuci√≥n de sentencia",
                "¬øCu√°nto dura un juicio civil?",
                "Costas procesales"
            ],
            "C√≥digo Procesal Penal": [
                "¬øC√≥mo hacer una denuncia penal?",
                "Derechos del imputado",
                "¬øQu√© es la prisi√≥n preventiva?",
                "Proceso abreviado vs ordinario",
                "Papel del fiscal en el proceso",
                "¬øQu√© es la querella?",
                "Derechos de la v√≠ctima",
                "Investigaci√≥n preliminar"
            ],
            "C√≥digo de la Ni√±ez y la Adolescencia": [
                "Derechos fundamentales del ni√±o",
                "¬øC√≥mo adoptar un menor?",
                "Protecci√≥n contra el trabajo infantil",
                "Menor infractor y medidas",
                "Tutela y curatela de menores",
                "¬øQu√© es la patria potestad?",
                "Maltrato infantil y denuncia",
                "Educaci√≥n obligatoria"
            ],
            "C√≥digo Aduanero": [
                "¬øC√≥mo importar mercader√≠a?",
                "Aranceles de importaci√≥n",
                "¬øQu√© es la zona franca?",
                "Proceso de exportaci√≥n",
                "Declaraci√≥n aduanera",
                "R√©gimen de equipaje",
                "¬øQu√© es el contrabando?",
                "Tributos aduaneros"
            ],
            "C√≥digo Sanitario": [
                "Regulaci√≥n de medicamentos",
                "Habilitaci√≥n de establecimientos de salud",
                "¬øQu√© es la farmacovigilancia?",
                "Control sanitario de alimentos",
                "Profesionales de la salud",
                "Emergencias sanitarias",
                "¬øC√≥mo denunciar mala praxis?",
                "Vacunaci√≥n obligatoria"
            ],
            "C√≥digo de Organizaci√≥n Judicial": [
                "¬øC√≥mo est√° organizado el Poder Judicial?",
                "Competencia de los juzgados",
                "¬øQu√© es la Corte Suprema?",
                "Funciones del secretario judicial",
                "¬øC√≥mo ser juez en Paraguay?",
                "Instancias judiciales",
                "Consejo de la Magistratura",
                "Fueros especiales"
            ]
        }
    
    def registrar_consulta(self, consulta: str):
        """Registra una consulta para tracking de frecuencia"""
        consulta_normalizada = self._normalizar_consulta(consulta)
        if consulta_normalizada:
            self.consultas_frecuentes[consulta_normalizada] = self.consultas_frecuentes.get(consulta_normalizada, 0) + 1
    
    def _normalizar_consulta(self, consulta: str) -> str:
        """Normaliza consulta para tracking"""
        if len(consulta) < 10 or len(consulta) > 200:
            return ""
        
        # Limpiar y normalizar
        normalizada = consulta.lower().strip()
        normalizada = re.sub(r'[^\w\s]', ' ', normalizada)
        normalizada = re.sub(r'\s+', ' ', normalizada)
        
        return normalizada
    
    def obtener_sugerencias(self, texto_parcial: str = "", limite: int = 8) -> List[Dict]:
        """
        Obtiene sugerencias inteligentes basadas en texto parcial
        """
        sugerencias = []
        texto_lower = texto_parcial.lower()
        
        # Si hay texto parcial, buscar coincidencias
        if texto_parcial and len(texto_parcial) >= 3:
            # Buscar en sugerencias por c√≥digo
            for codigo, lista_sugerencias in self.sugerencias_por_codigo.items():
                for sugerencia in lista_sugerencias:
                    if texto_lower in sugerencia.lower():
                        sugerencias.append({
                            "texto": sugerencia,
                            "codigo": codigo,
                            "tipo": "coincidencia",
                            "relevancia": self._calcular_relevancia(texto_lower, sugerencia.lower())
                        })
            
            # Buscar en consultas frecuentes
            for consulta_freq, contador in self.consultas_frecuentes.items():
                if texto_lower in consulta_freq and contador >= 2:
                    sugerencias.append({
                        "texto": consulta_freq.title(),
                        "codigo": "Consultas Frecuentes",
                        "tipo": "frecuente",
                        "relevancia": min(contador / 10, 1.0),
                        "veces_consultada": contador
                    })
        
        else:
            # Sin texto parcial, devolver sugerencias populares
            sugerencias_populares = [
                {"texto": "¬øQu√© dice el art√≠culo 32 sobre matrimonio?", "codigo": "C√≥digo Civil", "tipo": "popular"},
                {"texto": "¬øC√≥mo hacer una denuncia penal?", "codigo": "C√≥digo Procesal Penal", "tipo": "popular"},
                {"texto": "¬øCu√°nto es la indemnizaci√≥n por despido?", "codigo": "C√≥digo Laboral", "tipo": "popular"},
                {"texto": "Requisitos para contraer matrimonio", "codigo": "C√≥digo Civil", "tipo": "popular"},
                {"texto": "¬øC√≥mo denunciar violencia dom√©stica?", "codigo": "C√≥digo Penal", "tipo": "popular"},
                {"texto": "Derechos del trabajador en Paraguay", "codigo": "C√≥digo Laboral", "tipo": "popular"},
                {"texto": "¬øC√≥mo iniciar un proceso de divorcio?", "codigo": "C√≥digo Civil", "tipo": "popular"},
                {"texto": "¬øQu√© hacer en caso de estafa?", "codigo": "C√≥digo Penal", "tipo": "popular"}
            ]
            sugerencias.extend(sugerencias_populares)
        
        # Ordenar por relevancia y limitar
        if texto_parcial:
            sugerencias.sort(key=lambda x: x.get('relevancia', 0), reverse=True)
        
        return sugerencias[:limite]
    
    def _calcular_relevancia(self, texto_busqueda: str, sugerencia: str) -> float:
        """Calcula relevancia entre texto buscado y sugerencia"""
        # Coincidencia exacta
        if texto_busqueda in sugerencia:
            base_score = 0.8
        else:
            base_score = 0.3
        
        # Bonus por palabras en com√∫n
        palabras_busqueda = set(texto_busqueda.split())
        palabras_sugerencia = set(sugerencia.split())
        palabras_comunes = palabras_busqueda & palabras_sugerencia
        
        if len(palabras_busqueda) > 0:
            bonus = len(palabras_comunes) / len(palabras_busqueda) * 0.3
        else:
            bonus = 0
        
        return min(base_score + bonus, 1.0)
    
    def obtener_sugerencias_por_codigo(self, codigo_nombre: str, limite: int = 5) -> List[str]:
        """Obtiene sugerencias espec√≠ficas de un c√≥digo legal"""
        return self.sugerencias_por_codigo.get(codigo_nombre, [])[:limite]
    
    def get_stats(self) -> Dict:
        """Estad√≠sticas del sistema de sugerencias"""
        total_consultas_registradas = sum(self.consultas_frecuentes.values())
        consultas_unicas = len(self.consultas_frecuentes)
        
        # Top 5 consultas m√°s frecuentes
        top_consultas = sorted(
            self.consultas_frecuentes.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        return {
            "total_consultas_registradas": total_consultas_registradas,
            "consultas_unicas": consultas_unicas,
            "total_sugerencias_base": sum(len(sug) for sug in self.sugerencias_por_codigo.values()),
            "codigos_con_sugerencias": len(self.sugerencias_por_codigo),
            "top_consultas_frecuentes": [
                {"consulta": consulta, "veces": veces}
                for consulta, veces in top_consultas
            ]
        }

# ========== INSTANCIA GLOBAL DEL SISTEMA DE SUGERENCIAS ==========
sugerencias_manager = SugerenciasManager()

# ========== INSTANCIA GLOBAL DEL CACHE ==========
cache_manager = CacheManager(max_memory_mb=100)

# ========== INSTANCIA GLOBAL DEL CIRCUIT BREAKER ==========
circuit_breaker = CircuitBreaker()

# ========== RETRY LOGIC ROBUSTO ==========
def llamar_openai_con_retry(modelo: str, mensajes: list, max_tokens: int = 300, timeout: int = 25):
    """
    Llamada a OpenAI con retry logic robusto
    3 intentos con backoff exponencial: 0s, 2s, 4s
    """
    import time
    
    for intento in range(1, 4):  # 3 intentos: 1, 2, 3
        try:
            logger.info(f"üîÑ OpenAI intento {intento}/3 - Modelo: {modelo}")
            
            start_time = time.time()
            response = openai_client.chat.completions.create(
                model=modelo,
                messages=mensajes,
                temperature=0.1,
                max_tokens=max_tokens,
                presence_penalty=0,
                frequency_penalty=0,
                timeout=timeout
            )
            
            tiempo_respuesta = time.time() - start_time
            logger.info(f"‚úÖ OpenAI exitoso en intento {intento} - {tiempo_respuesta:.2f}s")
            return response
            
        except Exception as e:
            error_msg = str(e)[:100]
            logger.warning(f"‚ö†Ô∏è OpenAI intento {intento}/3 fall√≥: {error_msg}")
            
            # Si no es el √∫ltimo intento, esperar antes de reintentar
            if intento < 3:
                tiempo_espera = 2 ** (intento - 1)  # 0s, 2s, 4s
                if tiempo_espera > 0:
                    logger.info(f"‚è≥ Esperando {tiempo_espera}s antes del siguiente intento...")
                    time.sleep(tiempo_espera)
            else:
                # √öltimo intento fallido
                logger.error(f"‚ùå OpenAI fall√≥ despu√©s de 3 intentos: {error_msg}")
                raise e  # Re-lanzar la excepci√≥n para que el Circuit Breaker la maneje

# ========== NUEVO: GENERADOR DE EVENTOS DE PROCESAMIENTO ==========
async def generar_eventos_procesamiento(pregunta: str, collection_name: str):
    """
    Generador de Server-Sent Events para mostrar progreso en tiempo real
    """
    def crear_evento(tipo: str, mensaje: str, progreso: int = 0):
        return f"data: {json.dumps({'tipo': tipo, 'mensaje': mensaje, 'progreso': progreso, 'timestamp': datetime.now().isoformat()})}\n\n"
    
    try:
        # Evento 1: Inicio
        yield crear_evento("inicio", "üèõÔ∏è COLEPA procesando consulta legal...", 10)
        await asyncio.sleep(0.1)
        
        # Evento 2: Clasificaci√≥n
        yield crear_evento("clasificacion", "üß† Clasificando consulta en c√≥digos paraguayos...", 25)
        await asyncio.sleep(0.2)
        
        # Evento 3: Identificaci√≥n de c√≥digo
        codigo_nombre = "desconocido"
        for nombre, collection in MAPA_COLECCIONES.items():
            if collection == collection_name:
                codigo_nombre = nombre
                break
        
        yield crear_evento("codigo_identificado", f"üìö C√≥digo identificado: {codigo_nombre}", 40)
        await asyncio.sleep(0.1)
        
        # Evento 4: B√∫squeda
        numero_articulo = extraer_numero_articulo_mejorado(pregunta)
        if numero_articulo:
            yield crear_evento("busqueda", f"üîç Buscando Art√≠culo {numero_articulo} en base legal...", 60)
        else:
            yield crear_evento("busqueda", f"üîç Realizando b√∫squeda sem√°ntica en {codigo_nombre}...", 60)
        await asyncio.sleep(0.3)
        
        # Evento 5: Generaci√≥n
        yield crear_evento("generacion", "‚öñÔ∏è Generando respuesta jur√≠dica profesional...", 80)
        await asyncio.sleep(0.2)
        
        # Evento 6: Finalizaci√≥n
        yield crear_evento("completado", "‚úÖ Consulta legal procesada exitosamente", 100)
        
    except Exception as e:
        yield crear_evento("error", f"‚ùå Error en procesamiento: {str(e)[:100]}", 0)

# === MODELOS PYDANTIC ===
class MensajeChat(BaseModel):
    role: str = Field(..., pattern="^(user|assistant|system)$")
    content: str = Field(..., min_length=1, max_length=3000)
    timestamp: Optional[datetime] = None

class ConsultaRequest(BaseModel):
    historial: List[MensajeChat] = Field(..., min_items=1, max_items=20)
    metadatos: Optional[Dict[str, Any]] = None

class FuenteLegal(BaseModel):
    ley: str
    articulo_numero: str
    libro: Optional[str] = None
    titulo: Optional[str] = None

class ConsultaResponse(BaseModel):
    respuesta: str
    fuente: Optional[FuenteLegal] = None
    recomendaciones: Optional[List[str]] = None
    tiempo_procesamiento: Optional[float] = None
    es_respuesta_oficial: bool = True

class StatusResponse(BaseModel):
    status: str
    timestamp: datetime
    version: str
    servicios: Dict[str, str]
    colecciones_disponibles: int

# ========== NUEVOS MODELOS PARA M√âTRICAS ==========
class MetricasCalidad(BaseModel):
    consulta_id: str
    tiene_contexto: bool
    relevancia_contexto: float
    longitud_respuesta: int
    tiempo_procesamiento: float
    codigo_identificado: str
    articulo_encontrado: Optional[str] = None

# === CONFIGURACI√ìN DEL SISTEMA ===
MAPA_COLECCIONES = {
    "C√≥digo Aduanero": "colepa_aduanero_maestro",
    "C√≥digo Civil": "colepa_civil_maestro", 
    "C√≥digo Electoral": "colepa_electoral_maestro",
    "C√≥digo Laboral": "colepa_laboral_maestro",
    "C√≥digo de la Ni√±ez y la Adolescencia": "colepa_ninezadolescencia_maestro",
    "C√≥digo de Organizaci√≥n Judicial": "colepa_organizacion_judicial_maestro",
    "C√≥digo Penal": "colepa_penal_maestro",
    "C√≥digo Procesal Civil": "colepa_procesal_civil_maestro",
    "C√≥digo Procesal Penal": "colepa_procesal_penal_maestro",
    "C√≥digo Sanitario": "colepa_sanitario_maestro"
}

PALABRAS_CLAVE_EXPANDIDAS = {
    "C√≥digo Civil": [
        "civil", "matrimonio", "divorcio", "propiedad", "contratos", "familia", 
        "herencia", "sucesi√≥n", "sociedad conyugal", "bien ganancial", "patria potestad",
        "tutela", "curatela", "adopci√≥n", "filiaci√≥n", "alimentos", "r√©gimen patrimonial",
        "esposo", "esposa", "c√≥nyuge", "pareja", "hijos", "padres"
    ],
    "C√≥digo Penal": [
        "penal", "delito", "crimen", "pena", "prisi√≥n", "robo", "homicidio", "hurto",
        "estafa", "violaci√≥n", "agresi√≥n", "lesiones", "amenaza", "extorsi√≥n", "secuestro",
        "narcotr√°fico", "corrupci√≥n", "fraude", "violencia dom√©stica", "femicidio",
        "pega", "golpea", "golpes", "maltrato", "abuso", "acoso", "persigue", "molesta",
        "choque", "chocaron", "atropello", "accidente", "atropell√≥"
    ],
    "C√≥digo Laboral": [
        "laboral", "trabajo", "empleado", "salario", "vacaciones", "despido", "contrato laboral",
        "indemnizaci√≥n", "aguinaldo", "licencia", "maternidad", "seguridad social", "sindicato",
        "huelga", "jornada laboral", "horas extras", "jubilaci√≥n", "accidente laboral",
        "jefe", "patr√≥n", "empleador", "trabajador", "sueldo"
    ],
    "C√≥digo Procesal Civil": [
        "proceso civil", "demanda", "juicio civil", "sentencia", "apelaci√≥n", "recurso",
        "prueba", "testigo", "peritaje", "embargo", "medida cautelar", "ejecuci√≥n",
        "da√±os", "perjuicios", "responsabilidad civil", "indemnizaci√≥n"
    ],
    "C√≥digo Procesal Penal": [
        "proceso penal", "acusaci√≥n", "juicio penal", "fiscal", "defensor", "imputado",
        "querella", "investigaci√≥n", "allanamiento", "detenci√≥n", "prisi√≥n preventiva",
        "denuncia", "denunciar", "comisar√≠a", "polic√≠a"
    ],
    "C√≥digo Aduanero": [
        "aduana", "aduanero", "importaci√≥n", "exportaci√≥n", "aranceles", "tributo aduanero", "mercanc√≠a",
        "declaraci√≥n aduanera", "r√©gimen aduanero", "zona franca", "contrabando", "dep√≥sito", "habilitaci√≥n"
    ],
    "C√≥digo Electoral": [
        "electoral", "elecciones", "voto", "candidato", "sufragio", "padr√≥n electoral",
        "tribunal electoral", "campa√±a electoral", "partido pol√≠tico", "referendum"
    ],
    "C√≥digo de la Ni√±ez y la Adolescencia": [
        "menor", "ni√±o", "adolescente", "tutela", "adopci√≥n", "menor infractor",
        "protecci√≥n integral", "derechos del ni√±o", "consejer√≠a", "medida socioeducativa",
        "hijo", "hija", "ni√±os", "ni√±as", "menores"
    ],
    "C√≥digo de Organizaci√≥n Judicial": [
        "judicial", "tribunal", "juez", "competencia", "jurisdicci√≥n", "corte suprema",
        "juzgado", "fuero", "instancia", "sala", "magistrado", "secretario judicial"
    ],
    "C√≥digo Sanitario": [
        "sanitario", "salud", "medicina", "hospital", "cl√≠nica", "medicamento",
        "profesional sanitario", "epidemia", "vacuna", "control sanitario"
    ]
}

# ========== CONFIGURACI√ìN DE TOKENS OPTIMIZADA CON L√çMITES DIN√ÅMICOS ==========
MAX_TOKENS_INPUT_CONTEXTO = 800      # Aumentado para art√≠culos largos espec√≠ficos
MAX_TOKENS_RESPUESTA = 300           # M√°ximo tokens para respuesta
MAX_TOKENS_SISTEMA = 180             # M√°ximo tokens para prompt sistema

# ========== CONFIGURACI√ìN ADICIONAL PARA TRUNCADO INTELIGENTE ==========
MAX_TOKENS_ARTICULO_UNICO = 1500     # L√≠mite especial para art√≠culos √∫nicos largos
PRIORIDAD_COHERENCIA_JURIDICA = True  # Preservar coherencia legal sobre l√≠mites estrictos

# ========== PROMPT PREMIUM COMPACTO ==========
INSTRUCCION_SISTEMA_LEGAL_PREMIUM = """
COLEPA - Asistente jur√≠dico Paraguay. Respuesta obligatoria:

**DISPOSICI√ìN:** [Ley + Art√≠culo espec√≠fico]
**FUNDAMENTO:** [Texto normativo textual]  
**APLICACI√ìN:** [C√≥mo aplica a la consulta]

M√°ximo 250 palabras. Solo use contexto proporcionado. Terminolog√≠a jur√≠dica precisa.
"""

# ========== NUEVA FUNCI√ìN: VALIDADOR DE CONTEXTO ==========
def validar_calidad_contexto(contexto: Optional[Dict], pregunta: str) -> tuple[bool, float]:
    """
    Valida si el contexto encontrado es realmente relevante para la pregunta.
    VERSI√ìN OPTIMIZADA para art√≠culos largos y espec√≠ficos
    Retorna (es_valido, score_relevancia)
    """
    if not contexto or not contexto.get("pageContent"):
        return False, 0.0
    
    try:
        texto_contexto = contexto.get("pageContent", "").lower()
        pregunta_lower = pregunta.lower()
        
        # ========== VALIDACI√ìN ESPEC√çFICA PARA ART√çCULOS NUMERADOS ==========
        # Si se pregunta por un art√≠culo espec√≠fico y el contexto lo contiene, es autom√°ticamente v√°lido
        numero_pregunta = extraer_numero_articulo_mejorado(pregunta)
        numero_contexto = contexto.get("numero_articulo")
        
        if numero_pregunta and numero_contexto:
            try:
                if int(numero_contexto) == numero_pregunta:
                    logger.info(f"‚úÖ Validaci√≥n DIRECTA - Art√≠culo {numero_pregunta} encontrado exactamente")
                    return True, 1.0  # Score perfecto para coincidencia exacta
            except (ValueError, TypeError):
                pass
        
        # ========== VALIDACI√ìN PARA C√ìDIGO ESPEC√çFICO ==========
        # Si se menciona un c√≥digo espec√≠fico y el contexto es de ese c√≥digo, es v√°lido
        codigos_mencionados = []
        for codigo_nombre in MAPA_COLECCIONES.keys():
            codigo_lower = codigo_nombre.lower()
            if codigo_lower in pregunta_lower or any(palabra in pregunta_lower for palabra in codigo_lower.split()):
                codigos_mencionados.append(codigo_nombre)
        
        nombre_ley_contexto = contexto.get("nombre_ley", "").lower()
        for codigo in codigos_mencionados:
            if codigo.lower() in nombre_ley_contexto:
                logger.info(f"‚úÖ Validaci√≥n por C√ìDIGO - {codigo} coincide con contexto")
                return True, 0.9  # Score alto para coincidencia de c√≥digo
        
        # ========== VALIDACI√ìN SEM√ÅNTICA MEJORADA ==========
        # Extraer palabras clave de la pregunta
        palabras_pregunta = set(re.findall(r'\b\w+\b', pregunta_lower))
        palabras_contexto = set(re.findall(r'\b\w+\b', texto_contexto))
        
        # Filtrar palabras muy comunes que no aportan relevancia
        palabras_comunes = {"el", "la", "los", "las", "de", "del", "en", "con", "por", "para", "que", "se", "es", "un", "una", "y", "o", "a", "al"}
        palabras_pregunta -= palabras_comunes
        palabras_contexto -= palabras_comunes
        
        if len(palabras_pregunta) == 0:
            return False, 0.0
            
        # Calcular intersecci√≥n
        interseccion = palabras_pregunta & palabras_contexto
        score_basico = len(interseccion) / len(palabras_pregunta)
        
        # ========== BONUS ESPEC√çFICOS PARA CONTENIDO LEGAL ==========
        
        # Bonus por palabras clave jur√≠dicas importantes
        palabras_juridicas = {"art√≠culo", "c√≥digo", "ley", "disposici√≥n", "norma", "legal", "establece", "dispone", "determina", "ordena", "proh√≠be"}
        bonus_juridico = len(interseccion & palabras_juridicas) * 0.15
        
        # Bonus por n√∫meros de art√≠culo coincidentes
        numeros_pregunta = set(re.findall(r'\d+', pregunta))
        numeros_contexto = set(re.findall(r'\d+', texto_contexto))
        bonus_numeros = len(numeros_pregunta & numeros_contexto) * 0.25
        
        # Bonus por palabras clave espec√≠ficas del contexto legal
        palabras_clave_contexto = contexto.get("palabras_clave", [])
        if isinstance(palabras_clave_contexto, list):
            palabras_clave_set = set(palabra.lower() for palabra in palabras_clave_contexto)
            bonus_palabras_clave = len(palabras_pregunta & palabras_clave_set) * 0.2
        else:
            bonus_palabras_clave = 0
        
        # Bonus por longitud del contexto (art√≠culos largos suelen ser m√°s completos)
        longitud_contexto = len(texto_contexto)
        if longitud_contexto > 1000:  # Art√≠culos largos y detallados
            bonus_longitud = 0.1
        elif longitud_contexto > 500:
            bonus_longitud = 0.05
        else:
            bonus_longitud = 0
        
        score_final = score_basico + bonus_juridico + bonus_numeros + bonus_palabras_clave + bonus_longitud
        
        # ========== UMBRALES AJUSTADOS POR TIPO DE CONSULTA ==========
        
        # Umbral m√°s bajo para consultas espec√≠ficas por n√∫mero de art√≠culo
        if numero_pregunta:
            umbral_minimo = 0.10  # MUY permisivo para art√≠culos espec√≠ficos (FIX CR√çTICO)
        # Umbral normal para consultas tem√°ticas
        elif any(codigo.lower() in pregunta_lower for codigo in MAPA_COLECCIONES.keys()):
            umbral_minimo = 0.15   # M√°s permisivo para consultas de c√≥digo espec√≠fico
        else:
            umbral_minimo = 0.20  # Menos estricto para consultas generales
        
        # El contexto debe tener contenido m√≠nimo (RELAJADO para art√≠culos espec√≠ficos)
        if numero_pregunta:
            contenido_minimo = len(texto_contexto.strip()) >= 20  # M√°s permisivo para art√≠culos espec√≠ficos
        else:
            contenido_minimo = len(texto_contexto.strip()) >= 50
        
        es_valido = score_final >= umbral_minimo and contenido_minimo
        
        # ========== LOGGING MEJORADO ==========
        logger.info(f"üéØ Validaci√≥n contexto MEJORADA:")
        logger.info(f"   üìä Score b√°sico: {score_basico:.3f}")
        logger.info(f"   ‚öñÔ∏è Bonus jur√≠dico: {bonus_juridico:.3f}")
        logger.info(f"   üî¢ Bonus n√∫meros: {bonus_numeros:.3f}")
        logger.info(f"   üîë Bonus palabras clave: {bonus_palabras_clave:.3f}")
        logger.info(f"   üìè Bonus longitud: {bonus_longitud:.3f}")
        logger.info(f"   üéØ Score FINAL: {score_final:.3f}")
        logger.info(f"   ‚úÖ Umbral requerido: {umbral_minimo:.3f}")
        logger.info(f"   üèõÔ∏è V√ÅLIDO: {es_valido}")
        
        return es_valido, score_final
        
    except Exception as e:
        logger.error(f"‚ùå Error validando contexto: {e}")
        return False, 0.0

# ========== NUEVA FUNCI√ìN: B√öSQUEDA MULTI-M√âTODO CON CACHE ==========
def buscar_con_manejo_errores(pregunta: str, collection_name: str) -> Optional[Dict]:
    """
    B√∫squeda robusta con m√∫ltiples m√©todos, validaci√≥n de calidad y CACHE INTELIGENTE.
    VERSI√ìN CON LOGGING DETALLADO
    """
    logger.info(f"üîç INICIANDO b√∫squeda para pregunta: '{pregunta[:100]}...'")
    logger.info(f"üìö Colecci√≥n: {collection_name}")
    
    # ========== CACHE NIVEL 2: VERIFICAR CONTEXTO EN CACHE ==========
    contexto_cached = cache_manager.get_contexto(pregunta, collection_name)
    if contexto_cached:
        logger.info("üöÄ CACHE HIT - Contexto recuperado del cache, evitando b√∫squeda costosa")
        return contexto_cached
    
    contexto_final = None
    metodo_exitoso = None
    
    # ========== M√âTODO 1: B√öSQUEDA POR N√öMERO DE ART√çCULO ==========
    numero_articulo = extraer_numero_articulo_mejorado(pregunta)
    logger.info(f"üî¢ N√∫mero extra√≠do: {numero_articulo}")
    
    if numero_articulo and VECTOR_SEARCH_AVAILABLE:
        try:
            logger.info(f"üéØ M√âTODO 1: B√∫squeda exacta por art√≠culo {numero_articulo}")
            
            # Intentar b√∫squeda con n√∫mero como string (coincide con Qdrant)
            contexto = buscar_articulo_por_numero(str(numero_articulo), collection_name)
            logger.info(f"üìÑ Resultado b√∫squeda por n√∫mero (string): {contexto is not None}")
            
            # Si falla como string, intentar como int
            if not contexto:
                logger.info(f"üîÑ Reintentando b√∫squeda por n√∫mero como int")
                contexto = buscar_articulo_por_numero(numero_articulo, collection_name)
                logger.info(f"üìÑ Resultado b√∫squeda por n√∫mero (int): {contexto is not None}")
            
            if contexto:
                logger.info(f"‚úÖ Contexto encontrado en M√©todo 1:")
                logger.info(f"   üìñ Ley: {contexto.get('nombre_ley', 'N/A')}")
                logger.info(f"   üìã Art√≠culo: {contexto.get('numero_articulo', 'N/A')}")
                logger.info(f"   üìè Longitud: {len(contexto.get('pageContent', ''))}")
                
                es_valido, score = validar_calidad_contexto(contexto, pregunta)
                if es_valido:
                    contexto_final = contexto
                    metodo_exitoso = f"B√∫squeda exacta Art. {numero_articulo}"
                    logger.info(f"‚úÖ M√©todo 1 EXITOSO - Score: {score:.2f}")
                else:
                    logger.warning(f"‚ö†Ô∏è M√©todo 1 - Contexto no v√°lido (Score: {score:.2f})")
            else:
                logger.warning(f"‚ùå M√©todo 1 - No se encontr√≥ art√≠culo {numero_articulo}")
                
        except Exception as e:
            logger.error(f"‚ùå Error en M√©todo 1: {e}")
    else:
        if not numero_articulo:
            logger.info("‚è≠Ô∏è M√©todo 1 OMITIDO - No se extrajo n√∫mero de art√≠culo")
        if not VECTOR_SEARCH_AVAILABLE:
            logger.info("‚è≠Ô∏è M√©todo 1 OMITIDO - Vector search no disponible")
    
    # ========== M√âTODO 2: B√öSQUEDA SEM√ÅNTICA ==========
    if not contexto_final and OPENAI_AVAILABLE and VECTOR_SEARCH_AVAILABLE:
        try:
            logger.info("üîç M√âTODO 2: B√∫squeda sem√°ntica con embeddings")
            
            # Optimizar consulta para embeddings
            consulta_optimizada = f"{pregunta} legislaci√≥n paraguay derecho"
            logger.info(f"üéØ Consulta optimizada: '{consulta_optimizada}'")
            
            embedding_response = openai_client.embeddings.create(
                model="text-embedding-ada-002",
                input=consulta_optimizada
            )
            query_vector = embedding_response.data[0].embedding
            logger.info(f"üßÆ Embedding generado: {len(query_vector)} dimensiones")
            
            contexto = buscar_articulo_relevante(query_vector, collection_name)
            logger.info(f"üìÑ Resultado b√∫squeda sem√°ntica: {contexto is not None}")
            
            if contexto:
                logger.info(f"‚úÖ Contexto encontrado en M√©todo 2:")
                logger.info(f"   üìñ Ley: {contexto.get('nombre_ley', 'N/A')}")
                logger.info(f"   üìã Art√≠culo: {contexto.get('numero_articulo', 'N/A')}")
                
                es_valido, score = validar_calidad_contexto(contexto, pregunta)
                if es_valido and score >= 0.4:  # Umbral m√°s alto para sem√°ntica
                    contexto_final = contexto
                    metodo_exitoso = f"B√∫squeda sem√°ntica (Score: {score:.2f})"
                    logger.info(f"‚úÖ M√©todo 2 EXITOSO - Score: {score:.2f}")
                else:
                    logger.warning(f"‚ö†Ô∏è M√©todo 2 - Contexto no v√°lido (Score: {score:.2f})")
            else:
                logger.warning(f"‚ùå M√©todo 2 - No se encontr√≥ contexto relevante")
                    
        except Exception as e:
            logger.error(f"‚ùå Error en M√©todo 2: {e}")
    else:
        logger.info("‚è≠Ô∏è M√©todo 2 OMITIDO - Condiciones no cumplidas")
    
    # ========== M√âTODO 3: B√öSQUEDA FALLBACK ==========
    if not contexto_final and numero_articulo and VECTOR_SEARCH_AVAILABLE:
        try:
            logger.info("üîÑ M√âTODO 3: B√∫squeda fallback por palabras clave")
            
            # Crear vector dummy y usar filtros m√°s amplios
            contexto = buscar_articulo_relevante([0.1] * 1536, collection_name)
            logger.info(f"üìÑ Resultado b√∫squeda fallback: {contexto is not None}")
            
            if contexto:
                es_valido, score = validar_calidad_contexto(contexto, pregunta)
                if es_valido and score >= 0.2:  # Umbral m√°s bajo para fallback
                    contexto_final = contexto
                    metodo_exitoso = f"B√∫squeda fallback (Score: {score:.2f})"
                    logger.info(f"‚úÖ M√©todo 3 EXITOSO - Score: {score:.2f}")
                else:
                    logger.warning(f"‚ö†Ô∏è M√©todo 3 - Contexto no v√°lido (Score: {score:.2f})")
            else:
                logger.warning(f"‚ùå M√©todo 3 - No se encontr√≥ contexto fallback")
                    
        except Exception as e:
            logger.error(f"‚ùå Error en M√©todo 3: {e}")
    else:
        logger.info("‚è≠Ô∏è M√©todo 3 OMITIDO - Condiciones no cumplidas")
    
    # ========== RESULTADO FINAL ==========
    if contexto_final:
        logger.info(f"üéâ CONTEXTO ENCONTRADO usando: {metodo_exitoso}")
        cache_manager.set_contexto(pregunta, collection_name, contexto_final)
        return contexto_final
    else:
        logger.error("‚ùå NING√öN M√âTODO encontr√≥ contexto v√°lido")
        logger.error(f"   üîç B√∫squeda realizada en: {collection_name}")
        logger.error(f"   üìù Pregunta: '{pregunta}'")
        logger.error(f"   üî¢ N√∫mero extra√≠do: {numero_articulo}")
        return None

# === CONFIGURACI√ìN DE FASTAPI ===
app = FastAPI(
    title="COLEPA - Asistente Legal Oficial",
    description="Sistema de consultas legales basado en la legislaci√≥n paraguaya",
    version="3.3.0-PREMIUM-CACHE",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://www.colepa.com",
        "https://colepa.com", 
        "https://colepa-demo-2.vercel.app",
        "http://localhost:3000",
        "http://localhost:8080"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# ========== M√âTRICAS EN MEMORIA PARA DEMO ==========
metricas_sistema = {
    "consultas_procesadas": 0,
    "contextos_encontrados": 0,
    "tiempo_promedio": 0.0,
    "ultima_actualizacion": datetime.now()
}

# === FUNCIONES AUXILIARES MEJORADAS ===
def extraer_numero_articulo_mejorado(texto: str) -> Optional[int]:
    """
    Extracci√≥n mejorada y m√°s precisa de n√∫meros de art√≠culo
    VERSI√ìN OPTIMIZADA para casos reales
    """
    texto_lower = texto.lower().strip()
    
    # Patrones m√°s espec√≠ficos y completos - ORDEN IMPORTANTE
    patrones = [
        r'art[i√≠]culo\s*(?:n[√∫u]mero\s*)?(\d+)',  # "art√≠culo 32", "art√≠culo n√∫mero 32"
        r'art\.?\s*(\d+)',                        # "art. 32", "art 32"
        r'art√≠culo\s*(\d+)',                      # "art√≠culo 32"
        r'articulo\s*(\d+)',                      # "articulo 32" (sin tilde)
        r'art\s+(\d+)',                           # "art 32"
        r'(?:^|\s)(\d+)(?:\s+del\s+c[√≥o]digo)',  # "32 del c√≥digo"
        r'(?:^|\s)(\d+)(?:\s|$)',                 # N√∫mero aislado (√∫ltimo recurso)
    ]
    
    logger.info(f"üîç Extrayendo n√∫mero de art√≠culo de: '{texto[:100]}...'")
    
    for i, patron in enumerate(patrones):
        matches = re.finditer(patron, texto_lower)
        for match in matches:
            try:
                numero = int(match.group(1))
                if 1 <= numero <= 9999:  # Rango razonable para art√≠culos
                    logger.info(f"‚úÖ N√∫mero de art√≠culo extra√≠do: {numero} con patr√≥n {i+1}: {patron}")
                    return numero
                else:
                    logger.warning(f"‚ö†Ô∏è N√∫mero fuera de rango: {numero}")
            except (ValueError, IndexError):
                logger.warning(f"‚ö†Ô∏è Error procesando match: {match.group(1) if match else 'None'}")
                continue
    
    logger.warning(f"‚ùå No se encontr√≥ n√∫mero de art√≠culo v√°lido en: '{texto[:50]}...'")
    return None

def clasificar_consulta_inteligente(pregunta: str) -> str:
    """
    Clasificaci√≥n inteligente mejorada con mejor scoring
    """
    pregunta_lower = pregunta.lower()
    scores = {}
    
    # B√∫squeda por palabras clave con peso ajustado
    for ley, palabras in PALABRAS_CLAVE_EXPANDIDAS.items():
        score = 0
        for palabra in palabras:
            if palabra in pregunta_lower:
                # Mayor peso para coincidencias exactas de palabras completas
                if f" {palabra} " in f" {pregunta_lower} ":
                    score += 5
                elif palabra in pregunta_lower:
                    score += 2
        
        if score > 0:
            scores[ley] = score
    
    # B√∫squeda por menciones expl√≠citas de c√≥digos (peso muy alto)
    for ley in MAPA_COLECCIONES.keys():
        ley_lower = ley.lower()
        # Buscar nombre completo
        if ley_lower in pregunta_lower:
            scores[ley] = scores.get(ley, 0) + 20
        
        # Buscar versiones sin "c√≥digo"
        ley_sin_codigo = ley_lower.replace("c√≥digo ", "").replace("c√≥digo de ", "")
        if ley_sin_codigo in pregunta_lower:
            scores[ley] = scores.get(ley, 0) + 15
    
    # Patrones espec√≠ficos mejorados para casos reales
    patrones_especiales = {
        r'violen(cia|to|tar)|agre(si√≥n|dir)|golpe|maltrato|femicidio|pega|abuso': "C√≥digo Penal",
        r'matrimonio|divorcio|esposo|esposa|c√≥nyuge|familia|pareja': "C√≥digo Civil", 
        r'trabajo|empleo|empleado|jefe|patr√≥n|salario|sueldo|laboral': "C√≥digo Laboral",
        r'menor|ni√±o|ni√±a|adolescente|hijo|hija|adopci√≥n': "C√≥digo de la Ni√±ez y la Adolescencia",
        r'elecci√≥n|elecciones|voto|votar|candidato|pol√≠tico|electoral': "C√≥digo Electoral",
        r'choque|chocaron|atropello|atropell√≥|accidente|da√±os|perjuicios': "C√≥digo Procesal Civil",
        r'denuncia|fiscal|delito|acusado|penal|proceso penal|comisar√≠a': "C√≥digo Procesal Penal",
        r'aduana|aduanero|importa|exporta|mercanc√≠a|dep√≥sito': "C√≥digo Aduanero",
        r'salud|medicina|m√©dico|hospital|sanitario': "C√≥digo Sanitario",
        r'acoso|persigue|molesta|hostiga': "C√≥digo Penal"
    }
    
    for patron, ley in patrones_especiales.items():
        if re.search(patron, pregunta_lower):
            scores[ley] = scores.get(ley, 0) + 12
    
    # Determinar la mejor clasificaci√≥n
    if scores:
        mejor_ley = max(scores.keys(), key=lambda k: scores[k])
        score_final = scores[mejor_ley]
        logger.info(f"üìö Consulta clasificada como: {mejor_ley} (score: {score_final})")
        return MAPA_COLECCIONES[mejor_ley]
    
    # Default: C√≥digo Civil (m√°s general)
    logger.info("üìö Consulta no clasificada espec√≠ficamente, usando C√≥digo Civil por defecto")
    return MAPA_COLECCIONES["C√≥digo Civil"]

# ========== FUNCI√ìN CLASIFICACI√ìN CON CACHE NIVEL 1 ==========
def clasificar_consulta_con_ia_robusta(pregunta: str) -> str:
    """
    S√öPER ENRUTADOR CON CACHE: Clasificaci√≥n robusta usando IA con l√≠mites de tokens y cache inteligente
    """
    # ========== CACHE NIVEL 1: VERIFICAR CLASIFICACI√ìN EN CACHE ==========
    clasificacion_cached = cache_manager.get_clasificacion(pregunta)
    if clasificacion_cached:
        logger.info(f"üöÄ CACHE HIT - Clasificaci√≥n: {clasificacion_cached}")
        return clasificacion_cached
    
    # ========== CIRCUIT BREAKER: VERIFICAR ESTADO DE SERVICIOS ==========
    circuit_breaker.reset_if_needed()
    
    if not OPENAI_AVAILABLE or not openai_client or circuit_breaker.should_skip_openai():
        logger.warning("‚ö†Ô∏è OpenAI no disponible o circuit breaker activo - Usando clasificaci√≥n b√°sica")
        resultado = clasificar_consulta_inteligente(pregunta)
        cache_manager.set_clasificacion(pregunta, resultado)
        return resultado
    
    # PROMPT ULTRA-COMPACTO PARA CLASIFICACI√ìN
    prompt_clasificacion = f"""Clasifica esta consulta legal paraguaya en uno de estos c√≥digos:

C√ìDIGOS:
1. C√≥digo Civil - matrimonio, divorcio, familia, propiedad, contratos
2. C√≥digo Penal - delitos, violencia, agresi√≥n, robo, homicidio  
3.
